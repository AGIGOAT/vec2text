python run.py --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --max_seq_length 128 --model_name_or_path google/mt5-base --embedder_model_name bge-m3 --num_repeat_tokens 16 --embedder_no_grad True --num_train_epochs 100 --max_eval_samples 500 --eval_steps 20000 --warmup_steps 10000 --bf16=1 --use_wandb=1 --use_frozen_embeddings_as_input True --experiment inversion_decoder_only --lr_scheduler_type constant_with_warmup --exp_group_name wiki-de --learning_rate 0.001 --output_dir ./saves/gtr-1 --save_steps 2000