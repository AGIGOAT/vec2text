{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4206ccac-9fc8-4a1b-b22f-b0bb6c1b4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f443afa7-bdbd-4c85-abf8-effe715d513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loading model from checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n",
      "Set train_args.dataloader_num_workers = 4\n",
      "[1] creating model & stuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/emb_inv_dpr/nq-train/cache-6af83e39f92acf9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/emb_inv_dpr/nq-dev/cache-8796b523fc1740bd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] tokenizing dataset & preprocessing embeddings\n",
      "[3] initializing trainer\n",
      "[4] getting ckpnt\n",
      "[5] loading ckpt /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the character of the skull, which are relatively smooth and untutored in the case of infant sutures. Nevertheless, the sutures\n",
      "to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "individual from the Southern Hemisphere to win the Winter Olympic relay gold medal, and was also part of the Australian Short Track team which won a\n",
      "individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "the same rights as men, and 75% agreed that they should be protected from discrimination. Among the other 15% are heterosexuals, no work\n",
      "the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0522304773330688,\n",
       " 'eval_bleu_score': 31.552541624779003,\n",
       " 'eval_accuracy': 0.7384440104166666,\n",
       " 'eval_perplexity': 2.8640321578446315,\n",
       " 'eval_runtime': 16.2888,\n",
       " 'eval_samples_per_second': 30.696,\n",
       " 'eval_steps_per_second': 0.246}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import analyze_utils\n",
    "\n",
    "# https://wandb.ai/jack-morris/emb-inv-1/runs/ebb31d91810c4b62d2b55b5382e8c7ea/logs?workspace=user-jxmorris12\n",
    "checkpoint_folder = '/home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea'\n",
    "args_str = '--per_device_train_batch_size 128 --per_device_eval_batch_size 128 --max_seq_length 32 --model_name_or_path t5-base --embedder_model_name gtr_base --num_repeat_tokens 16 --embedder_no_grad True --exp_group_name mar17-baselines --learning_rate 0.0003 --freeze_strategy none --embedder_fake_with_zeros False --use_frozen_embeddings_as_input False --num_train_epochs 24 --max_eval_samples 500 --eval_steps 25000 --warmup_steps 100000 --bf16=1 --use_wandb=1'\n",
    "trainer = analyze_utils.load_inversion_model_and_trainer(checkpoint_folder, args_str)\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27fe10bd-0db1-4aaa-a4f8-e85aebfd41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "trainer.args.per_device_eval_batch_size = 128\n",
    "\n",
    "d = trainer.get_eval_dataloader()\n",
    "batch = next(iter(d))\n",
    "batch['input_ids'].shape\n",
    "\n",
    "torch._dynamo.config.verbose = True\n",
    "trainer.model.eval()\n",
    "\n",
    "gen_kwargs = trainer.gen_kwargs\n",
    "# gen_kwargs[\"temperature\"] = 1e-18\n",
    "# gen_kwargs[\"do_sample\"] = True\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "input_embs = trainer.model.call_embedding_model(\n",
    "    input_ids=batch['embedder_input_ids'].cuda(),\n",
    "    attention_mask=batch['embedder_attention_mask'].cuda(),\n",
    ")\n",
    "\n",
    "all_data = []\n",
    "outputs = trainer.model.generate(\n",
    "    inputs={k: v.cuda() for k,v in batch.items()},\n",
    "    generation_kwargs=gen_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a16d0a5-809b-4461-92ab-4b4a6fba3d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    8,   337,  2166,    38,  2541,   151,     6,   298, 13914, 15788,\n",
       "           26,     5,  5433,     6,   431,  7561,  4686,    24,    79,   225,\n",
       "           36,  5046,    45,  6940,  9192,   257,     5,   209,  5170,    13,\n",
       "          454,     1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61e2709f-5487-4af0-af4e-88111bc8793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    12,     8,  ...,     8,  2629, 10471],\n",
       "        [    0,   928,    45,  ...,   751,     3,     9],\n",
       "        [    0,     8,   337,  ...,     6,   150,   161],\n",
       "        ...,\n",
       "        [    0,    28,     3,  ...,   731,  2812,  2069],\n",
       "        [    0,  2144,  2345,  ...,   121,  2144,  2345],\n",
       "        [    0,    13,  7482,  ...,     6,   186,   999]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30c465bb-5007-45c8-8916-de74cab5e022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   12,     8,  1848,  ...,  2629, 10471,     1],\n",
       "        [  928,    45,     8,  ...,     3,     9,     1],\n",
       "        [    8,   337,  2166,  ...,   150,   161,     1],\n",
       "        ...,\n",
       "        [   28,     3,     9,  ...,  2812,  2069,     1],\n",
       "        [ 2144,  2345,    41,  ...,  2144,  2345,     1],\n",
       "        [   13,  7482,   291,  ...,   186,   999,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f1d654-980f-4358-9934-674750e8e87a",
   "metadata": {},
   "source": [
    "## checking likelihood of generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3347b314-1eeb-4ec4-9468-9604f0947da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix bos/eos tokens to align between input_ids and outputs\n",
    "new_outputs = torch.cat((outputs[:, 1:], torch.ones((len(outputs), 1), dtype=outputs.dtype, device=outputs.device)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e607d17b-ab0e-4678-bf5f-8ee1a0ed04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "inputs_embeds, attention_mask = trainer.model.embed_and_project(\n",
    "    embedder_input_ids=batch[\"embedder_input_ids\"].cuda(),\n",
    "    embedder_attention_mask=batch[\"embedder_attention_mask\"].cuda(),\n",
    ")\n",
    "true_outputs = trainer.model.encoder_decoder(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=batch[\"input_ids\"].cuda(),\n",
    ")\n",
    "pred_outputs = trainer.model.encoder_decoder(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=new_outputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7466b452-67d5-4732-b877-a35c169ef2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true data has ppl under model: 2.8347514997989016\n",
      "sampled data has ppl under model: 1.7082817983083902\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f\"true data has ppl under model: {math.exp(true_outputs.loss)}\")\n",
    "print(f\"sampled data has ppl under model: {math.exp(pred_outputs.loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958c73-de45-4f0b-b254-f1a1a8d41fe7",
   "metadata": {},
   "source": [
    "## checking with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4d7e7b-f090-4605-85ec-2a69ddc49a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mgen_kwargs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_text_with_noise\u001b[39m(embedder_input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, embedder_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor, alpha: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m      4\u001b[0m     inputs_embeds, attention_mask \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_and_project(\n\u001b[1;32m      5\u001b[0m         embedder_input_ids\u001b[38;5;241m=\u001b[39membedder_input_ids\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      6\u001b[0m         embedder_attention_mask\u001b[38;5;241m=\u001b[39membedder_attention_mask\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "gen_kwargs = trainer.gen_kwargs\n",
    "\n",
    "def sample_text_with_noise(embedder_input_ids: torch.Tensor, embedder_attention_mask: torch.Tensor, alpha: float):\n",
    "    inputs_embeds, attention_mask = trainer.model.embed_and_project(\n",
    "        embedder_input_ids=embedder_input_ids.cuda(),\n",
    "        embedder_attention_mask=embedder_attention_mask.cuda(),\n",
    "    )\n",
    "    inputs_embeds += torch.randn(inputs_embeds.shape, device=inputs_embeds.device) * alpha\n",
    "    return trainer.model.encoder_decoder.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "sample_text_with_noise(\n",
    "    embedder_input_ids=batch[\"embedder_input_ids\"],\n",
    "    embedder_attention_mask=batch[\"embedder_attention_mask\"],\n",
    "    alpha=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd7a90-91f0-422d-a556-c0cf36fe6f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
