{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ba1a0a-9b6b-4964-9951-09d0ab79e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bdc734-9eca-48b1-83e7-8a8a7892a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer for analysis – setting --do_eval=1\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set train_args.dataloader_num_workers = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wre\n",
      "================ End trainer sanity check ================\n"
     ]
    }
   ],
   "source": [
    "import analyze_utils\n",
    "\n",
    "# inversion model: wandb.ai/jack-morris/emb-inv-1/runs/ebb31d91810c4b62d2b55b5382e8c7ea/logs?workspace=user-jxmorris12\n",
    "# cross-encoder: wandb.ai/jack-morris/emb-rerank-1/runs/7bd186119e1f4f789eaa0731fd7357a1/overview?workspace=user-jxmorris12\n",
    "checkpoint_folder = \"/home/jxm3/research/retrieval/inversion/saves/98b1418d38c3f9333b17ab20bff06ff9/\"\n",
    "\n",
    "trainer = analyze_utils.load_trainer(checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8a9966-7d30-4d72-b6a3-7f56729a455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = trainer.inversion_trainer.model\n",
    "model.use_frozen_embeddings_as_input = True\n",
    "\n",
    "fake_embedder_input_ids = torch.ones([1], device=trainer.args.device)\n",
    "fake_embedder_attention_mask = torch.ones([1], device=trainer.args.device)\n",
    "frozen_embeddings = torch.randn((1, 768), device=trainer.args.device)\n",
    "\n",
    "tt = model.embedder_tokenizer('This is a random sentence!', return_tensors='pt')\n",
    "tt = tt.to(trainer.args.device)\n",
    "\n",
    "output = model.forward(\n",
    "    embedder_input_ids=fake_embedder_input_ids,\n",
    "    embedder_attention_mask=fake_embedder_attention_mask,\n",
    "    frozen_embeddings=frozen_embeddings,\n",
    "    labels=tt['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f13866-a80d-437f-a43f-80b9a57fe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_embeddings.requires_grad = True\n",
    "\n",
    "frozen_embeddings = torch.randn((1, 768), device=trainer.args.device, requires_grad=True)\n",
    "opt = torch.optim.Adam(params=[frozen_embeddings], lr=1)\n",
    "\n",
    "tt = model.embedder_tokenizer('This is a random sentence!', return_tensors='pt')\n",
    "tt = tt.to(trainer.args.device)\n",
    "print('goal text:', model.embedder_tokenizer.decode(tt['input_ids']))\n",
    "\n",
    "num_steps = 1000\n",
    "log_interval = 20\n",
    "for step in range(num_steps):\n",
    "    output = model.forward(\n",
    "        embedder_input_ids=fake_embedder_input_ids,\n",
    "        embedder_attention_mask=fake_embedder_attention_mask,\n",
    "        frozen_embeddings=frozen_embeddings,\n",
    "        labels=tt['input_ids']\n",
    "    )\n",
    "    gen_tokens = model.generate(\n",
    "        inputs={\n",
    "            'embedder_input_ids': fake_embedder_input_ids,\n",
    "            'embedder_attention_mask': fake_embedder_attention_mask,\n",
    "            'frozen_embeddings': frozen_embeddings,\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            \"max_length\": 32,\n",
    "            \"early_stopping\": False,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": False,\n",
    "        },\n",
    "    )\n",
    "    gen_text = model.embedder_tokenizer.decode(gen_tokens[0])\n",
    "    if step % log_interval == 0:\n",
    "        print(f'step {step} loss {output.loss}')\n",
    "        print(f'\\tgen_text = {gen_text}')\n",
    "    output.loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if loss < 0.5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155b70a-15ac-434d-af9f-14c6931e23c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe09a417450b4c819aba2456685be5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal text: könnte predominantly rubbish advanced voiciordination Sängerfällig Historical bluntLeaving Restaurant rigid lawmakers Pole simptome convins Monitoring corect 4:00 Nicolerennen DentistryET semnificativ înlocui disciplin ianuarie bleiben nostri wahr</s>\n",
      "step 0 loss 11.014447212219238\n",
      "\tgen_text = <pad> singing could be asymptomatic Individual vocal flexibility Sängerliche Tätigkeit Bonnie Vorwärts Werbung Werbung Werbung Werbung Werbung Werbung Werbung Werbung Werbung Werbung Vorwärts Regelungen Zusammen\n",
      "step 20 loss 9.999894142150879\n",
      "\tgen_text = <pad> rigid Can singerături modul modul sonore sonore sonore sonore sonore Werbung Werbung Werbung Werbung Werbung Werbung Zionism Vorwärts et comportament comportament Suceava Suceava Polonia Polonia Polonia\n",
      "step 40 loss 9.297483444213867\n",
      "\tgen_text = <pad> nearly rigid Bonnie Peut sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate sanatate can be conservative candidate pruning and speaking convention\n",
      "step 60 loss 8.803192138671875\n",
      "\tgen_text = <pad> increasingly restrictive Bonnie Peut conservative fructe fructe vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne vorne nominalpflichtigpflichtigpflichtigpflichtig\n",
      "step 80 loss 8.421220779418945\n",
      "\tgen_text = <pad> increasingly conservative residual Dragnea vorne excesiv Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean Judeţean or Siezedik restriction\n",
      "step 100 loss 8.026032447814941\n",
      "\tgen_text = <pad> increasingly conservative rendering rubbish excessive Sängerhinweis Nicolae Nicolae Națională Judeţean Judeţean destul Sängerhinweis Nicolae Nicolae Națională Judeţean destul Sprechenden Nicolae Nicolae și Zaporozhian\n",
      "step 120 loss 7.6997599601745605\n",
      "\tgen_text = <pad> remainder Dumpsterământământământământământământământământământământământământământământământământământământământământământământ conservative candidates spinning spinning spinning spinning sfânt\n",
      "step 140 loss 7.580630302429199\n",
      "\tgen_text = <pad> remainder Dumpsterământământământământământământământământământământământământământământământământământământământământământământ spinning spinning spinning spinning candidate excessive sfânt\n",
      "step 160 loss 7.502612113952637\n",
      "\tgen_text = <pad> remainder Dumpster rubbish spinning Dumpster rubbish spinning Nicolaeși leaving incumbent SpeakeranNapoca to be more accurately left to entertain comedian SängeranNapoca to entertain comedian SängeranNapoca\n",
      "step 180 loss 7.327486515045166\n",
      "\tgen_text = <pad> remainder spinning rubbish rubbish incumbent Speakerassez incumbent Speakerassez incumbent Nicolae Nicolae Nicolaeși leaving polesitter positions comfortably excluded Dog buff buff buff buff buff buff buff buff buff\n",
      "step 200 loss 7.198986053466797\n",
      "\tgen_text = <pad> remainder would leave incumbent incumbent incumbent incumbent incumbent Dumpsteriolong rubbish cleaning Specialist polestriastria Cristiani lăsa lăsa lăsa lăsa lăsa lăsa lăsa lăsa lăsa lăsa lăsa lăsa\n",
      "step 220 loss 7.137022495269775\n",
      "\tgen_text = <pad> be predominantly rubbish rubbish demanding Sonntagassez leaving polesitter Cristianuire challenging incumbent motoristi Cristianuire challenging incumbent Moderatori Iulia Iulia Iulia Iulia Iulia Iulia Iulia Iulia Iulia\n",
      "step 240 loss 7.050289154052734\n",
      "\tgen_text = <pad> be conservative conservative Dictionary rubbish handling Sängerassez incumbent Sängerassez incumbent Poleklikliklikliklikliklikliklikliklikliklikliklikli, vendredi\n",
      "step 260 loss 6.940744876861572\n",
      "\tgen_text = <pad> is conservative Literary rubbish rubbish pruning Polelioniceşti excluded Sängertôt le vendredi strict Nicolae Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox Ortodox\n",
      "step 280 loss 6.849137783050537\n",
      "\tgen_text = <pad> conservative conservative Dictionary rubbish handling Sängertôt Poleniernier Sonntag rubbish cleaning Jugendliche letiritiritiritiritiritiritiritiritiritiritiritiri are officially excluded from\n",
      "step 300 loss 6.711790561676025\n",
      "\tgen_text = <pad> conservative conservative Dictionary rubbish spinning Sonntagassez blunt spinning Sonntagassez restrictive PoleNeurung restrictive PoleNeurung restrictive Medical Workers pushing the icon of Marilyn excluded from the menu\n",
      "step 320 loss 6.6120686531066895\n",
      "\tgen_text = <pad> conservative conservative conservative Dictionary spinning Noirassez simptome simptome simptome simptome simptome Cleaning detergent Cleaning elements Cleaning pole Cleaning elements Cleaning elements Cleaning elements Cleaning elements Cleaning elements Cleaning elements Cleaning\n",
      "step 340 loss 6.5964860916137695\n",
      "\tgen_text = <pad> leaves the predominant rubbish spinning Traditionalurmatoarele excluded Cleaning blunt excluded Cleaning gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit gasit\n",
      "step 360 loss 6.448266506195068\n",
      "\tgen_text = <pad> might leave the predominant remainder spinning gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat pole pole blocking noise\n",
      "step 380 loss 6.306707382202148\n",
      "\tgen_text = <pad> might leave the predominant remainder spinning gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi gasi adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat pole pole blocking noise\n",
      "step 400 loss 6.383068561553955\n",
      "\tgen_text = <pad> might leave the predominant candidate spinning Sonntagassez simptome simptome simptome adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat Politi Klienen rubbish cleaning cleaning cardiac arresting outside\n",
      "step 420 loss 6.444045066833496\n",
      "\tgen_text = <pad> might leave the predominant occupation leading românescpluplu simptome simptome simptome simptome simptome simptome simptome simptome rubbish rubbish Cleaning endorsement Policylenileni turning Jan Klienborg towards Son\n",
      "step 440 loss 6.333294868469238\n",
      "\tgen_text = <pad> might leave the predominant occupation candidate Cannesplutôt restricti disciplini trandafiri restricti disciplini saint dimanche dimanche dimanche dimanche dimanche dimanche dimanche dimanche Poleklikli\n",
      "step 460 loss 6.182084560394287\n",
      "\tgen_text = <pad> might leave the predominant occupation candidates Flexible simptome simptome simptome simptome simptome disciplin disciplin românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc\n",
      "step 480 loss 6.099597930908203\n",
      "\tgen_text = <pad> might restrict the predominant candidate leaving CNN Dentistry Servicii Servicii Servicii Servicii Servicii Servicii Servicii Servicii Servicii Servicii Servicii usoara usoara usoara usoara usoara usoara usoara usoara usoara usoara usoara usoara\n",
      "step 500 loss 6.1169328689575195\n",
      "\tgen_text = <pad> might restrict the incumbent speakers independence Polit dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti dispoziti\n",
      "step 520 loss 6.110471248626709\n",
      "\tgen_text = <pad> might restrict the incumbent speakers rubbish blunt stationary endorsement iubit iubit iubit iubit Sonntagnièrenière incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere incredere\n",
      "step 540 loss 6.067702770233154\n",
      "\tgen_text = <pad> consider this restrictive candidate rubbish blunt stationary duminica trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir« Dental shaping« Medical processing and cooking« Pole\n",
      "step 560 loss 5.9645819664001465\n",
      "\tgen_text = <pad> while other candidates restrict rubbish blunt stationary endorsement iubit iubit iubit iubit Sonntagnièrenière trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir trandafir\n",
      "step 580 loss 5.892343997955322\n",
      "\tgen_text = <pad> rubbish blunt stationary constient constient Sonntag Polearni spinning around disciplin disciplin diferit diferit diferit emergent«« Medical Officer leaving the most nutritious component, Dentistry Officer leaving\n",
      "step 600 loss 5.936361312866211\n",
      "\tgen_text = <pad> rubbish blunt stationary disciplin disciplin disciplin diferit diferit românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc românesc leaving the most recent\n",
      "step 620 loss 6.028800010681152\n",
      "\tgen_text = <pad> rubbish rubbish Polelection constient constient Sonntag spun Polelection constient constient SonararniarniLeaving the predominant medical candidate, Dentistry blunt blunt blunt blunt blunt blunt blunt blunt\n",
      "step 640 loss 6.060221195220947\n",
      "\tgen_text = <pad> rubbish rubbish Polelection voici simptome simptome simptome adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat adevarat spun spun spun leaving the northern medical examiner\n",
      "step 660 loss 5.99995756149292\n",
      "\tgen_text = <pad> rubbish rubbish Polelection voici simptome simptome simptome simptome slowly leaves the country leaving the predominant candidate Dentalouvriouvrian disciplini, Dentalouvrian disciplini, Dental\n",
      "step 680 loss 5.892004013061523\n",
      "\tgen_text = <pad> leaves the incumbent discipline PoledentsÜlection PoledentsÜlection Flexible unemployment blunt blunt stationary medical debris leaving the northern dumpster leaving the southern dumpster leaving Heidi Lau and\n",
      "step 700 loss 5.880309104919434\n",
      "\tgen_text = <pad> may include:urmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoarele Di rubbish Pole Position stationary endorsement candidate Heidi left the city around\n",
      "step 720 loss 5.94271993637085\n",
      "\tgen_text = <pad> may include:urmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoarele Di rubbish Pole Position conservative blocking out the city'\n",
      "step 740 loss 5.985157489776611\n",
      "\tgen_text = <pad> may include:urmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoarele\" Industry rubbish blocking candidate Dog sitting\n",
      "step 760 loss 5.9174113273620605\n",
      "\tgen_text = <pad> may include:urmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoarele\" Industry rubbish blocking the current position Dog Ny\n",
      "step 780 loss 5.756279945373535\n",
      "\tgen_text = <pad> may include:urmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoareleurmatoarele Politia conservative rubbish blocking out the city'\n",
      "step 800 loss 5.632513523101807\n",
      "\tgen_text = <pad> may include: voici simptome mundanearni Speaker slowly left the parliament cleaning and blocking speakers Heidi blunt simptome simptome mundanearni Speaker slowly left the parliament cleaning and blocking Pole\n",
      "step 820 loss 5.629038333892822\n",
      "\tgen_text = <pad> although unemployment blunt simptome simptome simptome slowly leaves the discipline predominantly cleansing detergent Cleaning Polenialan Monitoring Politibilibilan Monitoring Politibilan Monitoring Politibilaning\n",
      "step 840 loss 5.726855278015137\n",
      "\tgen_text = <pad> although simptome simptome simptome mundanely handles various positions Cleaning rubbish Cleaning Poleodontian blunt blunt blunt blunt blunt blunt blunt blunt blunt blunt blunt blunt blunt toothed\n",
      "step 860 loss 5.851834297180176\n",
      "\tgen_text = <pad> although simptome simptome simptome mundanely rendered various positions Cleaning rubbish Cleaning Poleodontian Monitoring blunt Cleaning candidate Monitoring Poleodontian Monitoring blunt Cleaning candidate Monitoring saint Julian\n",
      "step 880 loss 5.952883720397949\n",
      "\tgen_text = <pad> predominantly rubbish blunt blunt simptome simptome Monitoring Politana localitate localitate Politana localitate localitate Politana localitate localitate Nyanglingling or slowly retiring Officer but\n",
      "step 900 loss 5.9648637771606445\n",
      "\tgen_text = <pad> predominantly rubbish blunt blunt orthodontic simptome simptome Politana localitate localitate Politana localitate localitate Politana localitate localitate Monitoring and Cleaning Late Ripping Representative | Year\n",
      "step 920 loss 5.882711887359619\n",
      "\tgen_text = <pad> predominantly rubbish blunt blunt orthodontic simptome simptome Politana localitate localitate Politana localitate localitate Politana localitate localitate Pole Position stabilising turning the lens-clean\n",
      "step 940 loss 5.745837211608887\n",
      "\tgen_text = <pad> predominantly rubbish blunt blunt orthodontic simptome simptome Politana localitate localitate Politana localitate localitate Politana localitate localitate Polesupported turning the recent turning period of the\n",
      "step 960 loss 5.609038352966309\n",
      "\tgen_text = <pad> predominantly rubbish blunt biased simptome simptome simptome Politananaassezassez localitate localitate Politananaassezassezassez localitate localitate Cleaning Cleaning Savior Monitoring Officer | Month\n",
      "step 980 loss 5.533006191253662\n",
      "\tgen_text = <pad> considering this discipline predominantly rubbish blunt blunt blunt simptome simptome simptome Politana localitate localitate Politana localitate localitate Polesupported blunt blunt blunt blunt blunt blunt blunt blunt\n",
      "step 1000 loss 5.533718109130859\n",
      "\tgen_text = <pad> considering this discipline predominantly rubbish blunt blunt blunt simptome simptome simptome legislative trandafir trandafir Politana localitate localitate Politana localitate localitate localitate localitate Pole Position dependent Monitoring Officer\n",
      "step 1020 loss 5.595969200134277\n",
      "\tgen_text = <pad> consider this discipline predominantly rubbish rubbish blunt blunt sustinut sustinut sustinut Polit diferit diferit Sonntagassez limbaj diferit«« Cleaning Officer left controlling the most significant candidate of the month\n",
      "step 1040 loss 5.679955005645752\n",
      "\tgen_text = <pad> consider this discipline predominantly rubbish rubbish blunt blunt sustinut sustinut sustinut sustinut Polit diferit diferit Serviciibliebenbliebenblieben or Officer blunt blunt blunting left Parliament-controlled turning around\n",
      "step 1060 loss 5.734495639801025\n",
      "\tgen_text = <pad> consider this discipline predominantly rubbish rubbish blunt blunt sustinut sustinut sustinut sustinut Polit diferit diferit Serviciibliebenbliebenblieben or Officer slowly cleaning the month of November, but Heidi limbaj\n",
      "step 1080 loss 5.722413539886475\n",
      "\tgen_text = <pad> consider this discipline predominantly rubbish rubbish blunt sustinut sustinut sustinut Polit diferit diferit Serviciibliebenbliebenblieben or Officer blunt blunt Flexible Flexible Flexible Flexible Flexible Flexible Flexible Duration Duration of\n",
      "step 1100 loss 5.660885810852051\n",
      "\tgen_text = <pad> consider this discipline predominantly rubbish rubbish sustinut sustinut sustinut Polit diferit diferit Serviciibliebenbliebenblieben or Officer blunt blunt blunt Flexible Flexible Standing Cristian Jugendliche blunt blunt blunt blunting\n",
      "step 1120 loss 5.603227138519287\n",
      "\tgen_text = <pad> consider this discipline technically redundant: Officer distracted Monitoring Officer distracted Heiditrecerea rămân rământrecerea trandafiri anume anume anume anume anume anume anume anume anume anume sunet sunet sunet\n",
      "step 1140 loss 5.591025352478027\n",
      "\tgen_text = <pad> consider this discipline technically redundant: Officer distracted Monitoring Parliamentan sustinut sustinut Polebending Heidi Rougetan sustinut Sfint Sfint Sfint Sfinttrecereatrecerea rămân rămân duminica duminica duminica\n",
      "step 1160 loss 5.6088643074035645\n",
      "\tgen_text = <pad> consider this discipline technically mundane Officer stationary Sunt diferit diferit Heiditrecerea rămân diferit diferit Heiditrecerea rămân diferit Rougenier rămân diferit Polebending Lars Rottanan (9\n",
      "step 1180 loss 5.643307685852051\n",
      "\tgen_text = <pad> consider this discipline redundant Polebending Officer Heidi rămân rământrecereatrecerea Domnului Domnului Domnului Domnului rământrecereatrecerea conservative conservative considering actual time left behind cleaning the parliament's\n",
      "step 1200 loss 5.664949417114258\n",
      "\tgen_text = <pad> other positions: Dentistry distracted Officer Monitoring sustinut Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sunt astăzi rămânândând destul Polewindentan turning\n",
      "step 1220 loss 5.651752948760986\n",
      "\tgen_text = <pad> other positions: tradiţional rubbish cleaning Officer rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămânhaudierehaudiere Flexible disciplinarian turning Savior sensitive teenager turning\n",
      "step 1240 loss 5.603494167327881\n",
      "\tgen_text = <pad> other positions: tradiţional rubbish cleaning Officer rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămân rămânhaudierehaudiere Cleaning Hot Dog-ridden candidate of\n",
      "step 1260 loss 5.534648418426514\n",
      "\tgen_text = <pad> other positions Surveyodontian tradiţionalassezassezassezassez blunt blunt steering Officer distracted rookie rămân rămân rămân rămân vigilant Dental Cleaning localitate localitate localitate localitate localitate localitate localitate\n",
      "step 1280 loss 5.481177806854248\n",
      "\tgen_text = <pad> other positions Surveyodontian distracted Surveyodontian distracted Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint rotating Polebending monotone but actual\n",
      "step 1300 loss 5.457754611968994\n",
      "\tgen_text = <pad> other positions Surveyodontian distracted Surveyodontian distracted Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint Sfint rotating Pole-resistant turning Pole\n",
      "step 1320 loss 5.457987308502197\n",
      "\tgen_text = <pad> other positions Surveyodontian distracted Surveyodontian distracted Surveyodontian biased Bonnie blunt blunt blunt blunt blunt blunt blunt blunt bleiben bleiben bleiben bleiben bleiben bleiben bleiben\",\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "text = datasets.load_dataset('rotten_tomatoes')['train'][0]['text']\n",
    "tt = {\n",
    "    'input_ids': torch.randint(low=1, high=model.embedder_tokenizer.vocab_size, size=(1, 31), dtype=torch.long)\n",
    "}\n",
    "tt['input_ids'] = torch.cat((tt['input_ids'], torch.ones((1,1), dtype=torch.long)), dim=1)\n",
    "tt['attention_mask'] = torch.ones((1, 32))\n",
    "                            \n",
    "tt = {k: v.to(trainer.args.device) for k,v in tt.items()}\n",
    "\n",
    "print('goal text:', model.embedder_tokenizer.decode(tt['input_ids'][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    frozen_embeddings = model.call_embedding_model(\n",
    "        **tt\n",
    "    )\n",
    "frozen_embeddings.requires_grad = True\n",
    "# frozen_embeddings = torch.randn((1, 768), device=trainer.args.device, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam(params=[frozen_embeddings], lr=0.0001)\n",
    "\n",
    "num_steps = 2000\n",
    "log_interval = 20\n",
    "\n",
    "for step in range(0, num_steps):\n",
    "    output = model.forward(\n",
    "        embedder_input_ids=fake_embedder_input_ids,\n",
    "        embedder_attention_mask=fake_embedder_attention_mask,\n",
    "        frozen_embeddings=frozen_embeddings,\n",
    "        labels=tt['input_ids']\n",
    "    )\n",
    "    gen_tokens = model.generate(\n",
    "        inputs={\n",
    "            'embedder_input_ids': fake_embedder_input_ids,\n",
    "            'embedder_attention_mask': fake_embedder_attention_mask,\n",
    "            'frozen_embeddings': frozen_embeddings,\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            \"max_length\": 32,\n",
    "            \"early_stopping\": False,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": False,\n",
    "        },\n",
    "    )\n",
    "    gen_text = model.embedder_tokenizer.decode(gen_tokens[0])\n",
    "    if step % log_interval == 0:\n",
    "        print(f'step {step} loss {output.loss}')\n",
    "        print(f'\\tgen_text = {gen_text}')\n",
    "    output.loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if output.loss < 0.05: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14adbe6c-7311-4e02-8d82-b1e5a47d7caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c554425c908457ba08bf1840e9b7152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.load_dataset('rotten_tomatoes')['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a9eefbc-e189-45bc-bd3a-50f9b4a1341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e3a55dd9fb4960939dd2390e69fe4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_dataset('rotten_tomatoes')['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e559f521-3508-474c-9c59-f69a109f0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.499111145734787\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he's gonna make a conno\n",
      "step 20 loss 0.4835469722747803\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he's gonna make a conno\n",
      "step 40 loss 0.4662741720676422\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he's gonna make a conno\n",
      "step 60 loss 0.4493709206581116\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he's gonna make a conno\n",
      "step 80 loss 0.4321572184562683\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he is gonna make a connoisseur\n",
      "step 100 loss 0.41421860456466675\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he is gonna make a connoisseur\n",
      "step 120 loss 0.39724114537239075\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he is gonna make a conan and his\n",
      "step 140 loss 0.3814161419868469\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"\" and that he will make a conan and a hit\n",
      "step 160 loss 0.365671306848526\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he will make a hit with his \n",
      "step 180 loss 0.35062167048454285\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he will make a hit with his \n",
      "step 200 loss 0.33636102080345154\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he will make a hit with his \"\n",
      "step 220 loss 0.32267627120018005\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he will make a splash by putting\n",
      "step 240 loss 0.3104868531227112\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he's going to make a splash\n",
      "step 260 loss 0.30051562190055847\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he's going to make a splash\n",
      "step 280 loss 0.29132091999053955\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \"conan\" and that he's going to make a splash\n",
      "step 300 loss 0.28219136595726013\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 320 loss 0.2732362449169159\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 340 loss 0.26414722204208374\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 360 loss 0.25486069917678833\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 380 loss 0.24561402201652527\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 400 loss 0.23680564761161804\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 420 loss 0.22838732600212097\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 440 loss 0.22053754329681396\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 460 loss 0.21356816589832306\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 480 loss 0.20739509165287018\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 500 loss 0.2021246999502182\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 520 loss 0.19716861844062805\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 540 loss 0.19203035533428192\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 560 loss 0.18683753907680511\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 580 loss 0.1815498173236847\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 600 loss 0.17665353417396545\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 620 loss 0.17194610834121704\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 640 loss 0.167317733168602\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 660 loss 0.16280657052993774\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 680 loss 0.1583530306816101\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 700 loss 0.1540430337190628\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 720 loss 0.15024681389331818\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 740 loss 0.14698144793510437\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan\" and that he's going to make a splash\n",
      "step 760 loss 0.1441173106431961\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash\n",
      "step 780 loss 0.1416824609041214\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash\n",
      "step 800 loss 0.13982680439949036\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash\n",
      "step 820 loss 0.1384636014699936\n",
      "\tgen_text = <pad> the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_steps):\n\u001b[1;32m     23\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m     24\u001b[0m         embedder_input_ids\u001b[38;5;241m=\u001b[39mfake_embedder_input_ids,\n\u001b[1;32m     25\u001b[0m         embedder_attention_mask\u001b[38;5;241m=\u001b[39mfake_embedder_attention_mask,\n\u001b[1;32m     26\u001b[0m         frozen_embeddings\u001b[38;5;241m=\u001b[39mfrozen_embeddings,\n\u001b[1;32m     27\u001b[0m         labels\u001b[38;5;241m=\u001b[39mtt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedder_input_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_embedder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedder_attention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_embedder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrozen_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearly_stopping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_beams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     gen_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membedder_tokenizer\u001b[38;5;241m.\u001b[39mdecode(gen_tokens[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/research/retrieval/inversion/notebooks/../models.py:584\u001b[0m, in \u001b[0;36mInversionModel.generate\u001b[0;34m(self, inputs, generation_kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_decoder\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;66;03m# required: input embeddings\u001b[39;00m\n\u001b[1;32m    574\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs,\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# required: input embeddings\u001b[39;49;00m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# optional: input IDs (for starting generation).\u001b[39;49;00m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# typically not set unless generating prefixes for\u001b[39;49;00m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# reranking.\u001b[39;49;00m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1406\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1403\u001b[0m         )\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:2201\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2209\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1704\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1704\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1719\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1074\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1062\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1063\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:693\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    703\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m ):\n\u001b[1;32m    599\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 600\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    610\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:522\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    519\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(hidden_states))  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m value_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    526\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, key_value_states, past_key_value[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:506\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, key_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m past_key_value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;66;03m# checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    512\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "# text = datasets.load_dataset('rotten_tomatoes')['train'][0]['text']\n",
    "# tt = model.embedder_tokenizer(text, return_tensors='pt', max_length=32)\n",
    "# tt = tt.to(trainer.args.device)\n",
    "# print('goal text:', model.embedder_tokenizer.decode(tt['input_ids'][0]))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     frozen_embeddings = model.call_embedding_model(\n",
    "#         **tt\n",
    "#     )\n",
    "# frozen_embeddings.requires_grad = True\n",
    "# # frozen_embeddings = torch.randn((1, 768), device=trainer.args.device, requires_grad=True)\n",
    "\n",
    "# opt = torch.optim.Adam(params=[frozen_embeddings], lr=0.00001)\n",
    "\n",
    "num_steps = 2000\n",
    "log_interval = 20\n",
    "\n",
    "for step in range(0, num_steps):\n",
    "    output = model.forward(\n",
    "        embedder_input_ids=fake_embedder_input_ids,\n",
    "        embedder_attention_mask=fake_embedder_attention_mask,\n",
    "        frozen_embeddings=frozen_embeddings,\n",
    "        labels=tt['input_ids']\n",
    "    )\n",
    "    gen_tokens = model.generate(\n",
    "        inputs={\n",
    "            'embedder_input_ids': fake_embedder_input_ids,\n",
    "            'embedder_attention_mask': fake_embedder_attention_mask,\n",
    "            'frozen_embeddings': frozen_embeddings,\n",
    "        },\n",
    "        generation_kwargs={\n",
    "            \"max_length\": 32,\n",
    "            \"early_stopping\": False,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": False,\n",
    "        },\n",
    "    )\n",
    "    gen_text = model.embedder_tokenizer.decode(gen_tokens[0])\n",
    "    if step % log_interval == 0:\n",
    "        print(f'step {step} loss {output.loss}')\n",
    "        print(f'\\tgen_text = {gen_text}')\n",
    "    output.loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if output.loss < 0.05: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88c8a-3f9b-4c2c-a968-322251399034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
