{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1458fd3e-3b6d-4202-934b-e9569a6e8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "# sys.path = ['/home/jxm3/research/retrieval/inversion'] + sys.path\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5737925-85c5-4ee3-9fc9-03b05a91820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer for analysis – setting --do_eval=1\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set train_args.dataloader_num_workers = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2436a43d74d74f0ca4da46e7a5957bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b75f82d4243e7adab1e7b03cb758d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ad46f76ea42418ec26456c427c2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110540f7a65f47fd89094392ba8570f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wre\n",
      "================ End trainer sanity check ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import analyze_utils\n",
    "\n",
    "checkpoint_folder = \"/home/jxm3/research/retrieval/inversion/saves/98b1418d38c3f9333b17ab20bff06ff9/\"\n",
    "trainer = analyze_utils.load_trainer(checkpoint_folder).inversion_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6af68d6c-af20-4e6b-96d1-d4c1543de401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bda2b73e-9ddc-4510-aa8f-252dd052beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = next(iter(trainer.get_eval_dataloader(eval_dataset=trainer.eval_dataset[\"nq\"])))\n",
    "eval_batch = {k: v[None, 0] for k,v in eval_batch.items() } # take first item\n",
    "eval_batch = {k: v.to(trainer.args.device) for k,v in eval_batch.items() } # put on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25a487ef-7535-49a7-9540-66bf8e8e2293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact</s>']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.embedder_tokenizer.batch_decode(eval_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8c90d6d-5fd8-4836-affa-f4ed0bab9810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(probs @ trainer.model.embedder.get_input_embeddings().weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb2a40-b2a3-4bf7-bd8b-4a8be06922e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 32 - 1\n",
    "vocab_size = trainer.model.embedder.get_input_embeddings().weight.shape[0]\n",
    "logits = torch.rand((1, seq_length, vocab_size), device=trainer.args.device, requires_grad=True)\n",
    "attention_mask = torch.ones((1, seq_length), dtype=torch.long, device=trainer.args.device)\n",
    "trainer.model.embedder.get_input_embeddings().weight.requires_grad = False\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    true_embeddings = trainer.model.call_embedding_model(\n",
    "        input_ids=eval_batch[\"embedder_input_ids\"],\n",
    "        attention_mask=eval_batch[\"embedder_attention_mask\"],\n",
    "    )\n",
    "\n",
    "for _name, param in trainer.model.embedder.named_parameters(): param.requires_grad = False\n",
    "\n",
    "opt = torch.optim.Adam(params=[logits], lr=1e-1)\n",
    "\n",
    "# tt = trainer.model.embedder_tokenizer('This is a random sentence!', return_tensors='pt')\n",
    "# tt = tt.to(trainer.args.device)\n",
    "print('goal text:', trainer.model.embedder_tokenizer.decode(eval_batch['embedder_input_ids'][0]))\n",
    "\n",
    "eos_token_embed = trainer.model.embedder.embed_tokens(\n",
    "    torch.tensor([[1]], dtype=torch.long, device=trainer.args.device)\n",
    ")\n",
    "eos_token_embed.requires_grad = False\n",
    "\n",
    "num_steps = 10000\n",
    "log_interval = 100\n",
    "tau = 0.5\n",
    "for step in range(num_steps):\n",
    "    probs = torch.nn.functional.gumbel_softmax(logits, tau=tau)\n",
    "    # probs = torch.nn.functional.softmax(logits, -1)\n",
    "    inputs_embeds = probs @ trainer.model.embedder.get_input_embeddings().weight\n",
    "    \n",
    "    inputs_embeds = torch.cat(\n",
    "        (inputs_embeds, eos_token_embed),\n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    output = trainer.model.embedder(\n",
    "        inputs_embeds=inputs_embeds\n",
    "    )\n",
    "    attention_mask = torch.ones(inputs_embeds.shape[0:2], device=trainer.args.device)\n",
    "    embeddings = trainer.model._process_embedder_output(output, attention_mask)\n",
    "    \n",
    "    # loss = ((embeddings - true_embeddings) ** 2).sum(1)\n",
    "    loss = 1.0 - torch.nn.CosineSimilarity(1)(true_embeddings, embeddings)\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        gen_text = trainer.model.embedder_tokenizer.decode(logits[0].argmax(-1))\n",
    "        print(f'step {step} loss {loss}')\n",
    "        print(f'\\tgen_text = {gen_text}')\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if loss < 1e-3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f767eabb-3a38-466d-9223-6c03a4385148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_get_resized_embeddings',\n",
       " '_named_members',\n",
       " '_resize_token_embeddings',\n",
       " 'embed_tokens',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'get_position_embeddings',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'set_input_embeddings']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in dir(trainer.model.embedder) if 'emb' in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7eded-d9c7-47ac-a393-2ee71faa8f76",
   "metadata": {},
   "source": [
    "## try a shorter input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d464520-5cc1-477b-a5bc-edf319bff155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal text: hi there!</s>\n",
      "step 0 loss tensor([0.8693], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = Booksătă Sanchez individually\n",
      "step 100 loss tensor([0.6872], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = than HIGH Protect Nine\n",
      "step 200 loss tensor([0.5209], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = here strong lurk greeting\n",
      "step 300 loss tensor([0.3947], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = Here Sil hello Hai\n",
      "step 400 loss tensor([0.3651], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = HERE si hello hello\n",
      "step 500 loss tensor([0.2650], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = There hello hello there\n",
      "step 600 loss tensor([0.2291], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = HERE hello hello there\n",
      "step 700 loss tensor([0.2019], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 800 loss tensor([0.1593], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 900 loss tensor([0.2082], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1000 loss tensor([0.2111], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1100 loss tensor([0.1711], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello Hai there\n",
      "step 1200 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1300 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1400 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1500 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1600 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1700 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1800 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 1900 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2000 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2100 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2200 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2300 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2400 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2500 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2600 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2700 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2800 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 2900 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3000 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3100 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3200 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3300 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3400 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n",
      "step 3500 loss tensor([0.2135], device='cuda:0', grad_fn=<RsubBackward1>)\n",
      "\tgen_text = WHAT hello hello there\n"
     ]
    }
   ],
   "source": [
    "tt = trainer.model.embedder_tokenizer('hi there!', return_tensors='pt')\n",
    "tt = tt.to(trainer.args.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    true_embeddings = trainer.model.call_embedding_model(\n",
    "        input_ids=tt[\"input_ids\"],\n",
    "        attention_mask=tt[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "###\n",
    "seq_length = tt['input_ids'].shape[1]\n",
    "vocab_size = trainer.model.embedder.get_input_embeddings().weight.shape[0]\n",
    "logits = torch.rand((1, seq_length, vocab_size), device=trainer.args.device, requires_grad=True)\n",
    "attention_mask = torch.ones((1, seq_length), dtype=torch.long, device=trainer.args.device)\n",
    "trainer.model.embedder.get_input_embeddings().weight.requires_grad = False\n",
    "opt = torch.optim.Adam(params=[logits], lr=.05)\n",
    "####\n",
    "\n",
    "for _name, param in trainer.model.embedder.named_parameters(): param.requires_grad = False\n",
    "print('goal text:', trainer.model.embedder_tokenizer.decode(tt['input_ids'][0]))\n",
    "\n",
    "\n",
    "eos_token_embed = trainer.model.embedder.embed_tokens(\n",
    "    torch.tensor([[1]], dtype=torch.long, device=trainer.args.device)\n",
    ")\n",
    "eos_token_embed.requires_grad = False\n",
    "\n",
    "num_steps = 10000\n",
    "log_interval = 100\n",
    "tau = 1\n",
    "for step in range(num_steps):\n",
    "    probs = torch.nn.functional.gumbel_softmax(logits, tau=tau)\n",
    "    # probs = torch.nn.functional.softmax(logits, -1)\n",
    "    inputs_embeds = probs @ trainer.model.embedder.get_input_embeddings().weight\n",
    "    inputs_embeds = torch.cat(\n",
    "        (inputs_embeds, eos_token_embed),\n",
    "        dim=1\n",
    "    )\n",
    "    output = trainer.model.embedder(\n",
    "        inputs_embeds=inputs_embeds\n",
    "    )\n",
    "    attention_mask = torch.ones(inputs_embeds.shape[0:2], device=trainer.args.device)\n",
    "    embeddings = trainer.model._process_embedder_output(output, attention_mask)\n",
    "    \n",
    "    loss = 1.0 - torch.nn.CosineSimilarity(1)(true_embeddings, embeddings)\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        gen_text = trainer.model.embedder_tokenizer.decode(logits[0].argmax(-1))\n",
    "        print(f'step {step} loss {loss}')\n",
    "        print(f'\\tgen_text = {gen_text}')\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if loss < 1e-3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a6c94-b2f1-4ae6-8878-91493f1a0cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
