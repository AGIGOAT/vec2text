{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07094988-caaa-41b9-8574-aaedd553666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3276591a-1424-41e1-98d3-1dcbc3046e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alias openai_msmarco__msl128__100epoch__correct from /home/jxm3/research/retrieval/inversion/saves/d6ec9d5838a4ad3daeba636e5378a8a0...\n",
      "loading alias openai_msmarco__msl128__100epoch from /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "running openai on text_list of length 1, first element 'Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.'\n",
      "\tDecoded output shape ->  torch.Size([1, 69])\n",
      "\tDecoded output -> And the trolls, twilight, was smooty, And swiggy, smeared, And wispy, wisping, wily, wiz, wizz, wizzy, wizzie, wizy, wit.\n",
      "================ End trainer sanity check ================\n",
      "Froze 353779584 params from model type <class 'models.inversion.InversionModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "running openai on text_list of length 1, first element 'Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.'\n",
      "running openai on text_list of length 1, first element 'And the trolls, trolls, trolls, trolls, trolls, trolls, trolls, trolls, trolls, trolls, trolls,'\n",
      "running openai on text_list of length 1, first element 'Was brilliant, and witty, And wry, writhing, whirling, wit, twig, thy smoots, smeot, riss in the smolly.'\n",
      "\tDecoded output shape ->  torch.Size([1, 62])\n",
      "\tDecoded output -> Was brilliant, and witty, And wry, writhing, whirling, wit, twig, thy smoots, smeot, riss in the smolly.\n",
      "================ End trainer sanity check ================\n",
      "Loading hypotheses from path /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f/9d4a4d4b36da188a6e9dcb9736262823/0ed77465b20070e5_hypotheses.cache\n",
      "Loading hypotheses from path /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f/9d4a4d4b36da188a6e9dcb9736262823/fa5c492104fd5235_hypotheses.cache\n",
      "Loading hypotheses from path /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f/9d4a4d4b36da188a6e9dcb9736262823/927a7aa89af4e080_hypotheses.cache\n",
      "Loading hypotheses from path /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f/9d4a4d4b36da188a6e9dcb9736262823/d405c2908a2abede_hypotheses.cache\n",
      "Loading hypotheses from path /home/jxm3/research/retrieval/inversion/saves/f9abd65db4c4823264b133816d08612f/9d4a4d4b36da188a6e9dcb9736262823/35c1eff11b4e82d2_hypotheses.cache\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"openai_msmarco__msl128__100epoch__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5fcba1-049e-4981-a575-1ea53eea780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def bleu(t1: torch.Tensor, t2: torch.Tensor) -> float:\n",
    "    s1 = corr_trainer.embedder_tokenizer.decode(t1, skip_special_tokens=True)\n",
    "    s2 = corr_trainer.embedder_tokenizer.decode(t2, skip_special_tokens=True)\n",
    "    result = corr_trainer.metric_bleu.compute(\n",
    "        predictions=[s1], references=[s2]\n",
    "    )\n",
    "    return result[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f323075-3fa4-4e8d-a043-4ccf6f667126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "class EncourageTrueTokensLogitsProcessor(transformers.LogitsProcessor):\n",
    "    true_input_ids: torch.LongTensor\n",
    "    def __init__(self, true_input_ids: torch.LongTensor):\n",
    "        self.true_input_ids = true_input_ids\n",
    "        # self.gammas = torch.tensor([\n",
    "        #     random.random() * 10 for _ in range(len(true_input_ids))], \n",
    "        #     dtype=torch.float32\n",
    "        # )\n",
    "        \n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, next_token_logits: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        vocab_size = next_token_logits.shape[1]\n",
    "        \n",
    "        true_next_tokens = self.true_input_ids[:, input_ids.shape[1]-1]\n",
    "        fake_logits = torch.zeros_like(next_token_logits, device=next_token_logits.device)\n",
    "        # todo vectorize\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            fake_logits[i, true_next_tokens[i]] += random.random()\n",
    "        \n",
    "        # gamma = self.gammas[:, None].to(next_token_logits.device)\n",
    "        gamma = 8\n",
    "        logits = (next_token_logits + fake_logits * gamma).log_softmax(1)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        return logits\n",
    "################################################################################\n",
    "\n",
    "eval_dataset = corr_trainer.eval_dataset[\"msmarco\"].select(range(1024))\n",
    "bleus = []\n",
    "for batch in corr_trainer.get_eval_dataloader(eval_dataset=eval_dataset):\n",
    "    true_tokens_logits_processor = EncourageTrueTokensLogitsProcessor(\n",
    "        true_input_ids=batch[\"input_ids\"],\n",
    "    )\n",
    "\n",
    "    gen_kwargs[\"logits_processor\"] = transformers.LogitsProcessorList(\n",
    "        [\n",
    "            true_tokens_logits_processor,\n",
    "        ]\n",
    "    )\n",
    "    batch_on_device = { k: v.to(corr_trainer.args.device) for k,v in batch.items() }\n",
    "    outputs = corr_trainer.generate(\n",
    "        inputs=batch_on_device,\n",
    "        generation_kwargs=gen_kwargs,\n",
    "        num_recursive_steps=1\n",
    "    )\n",
    "    bleus.extend([bleu(t1, t2) for t1, t2 in zip(outputs, batch[\"input_ids\"])])\n",
    "\n",
    "print(sum(bleus) / len(bleus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f27787-1533-4be3-8e70-e432a4461c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot\n",
    "ax = sns.histplot(bleus, bins=25, color=sns.color_palette()[3])\n",
    "\n",
    "# ax.set_xlim([.85,1])\n",
    "# ax.set_ylim([0, 3_000])\n",
    "ax.set_xlabel(\"BLEU\")\n",
    "# ax.set_xscale(\"log\")\n",
    "\n",
    "ax.set_title(\"Hypothesis BLEU scores\")\n",
    "\n",
    "# save to PDF\n",
    "# plt.savefig(\"train_hypothesis_distance_sl128.pdf\", format=\"pdf\", bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a561ad3c-68fc-4cdf-8a8a-07f447a3fe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0283)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(bleus) >= 90).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3519f-2564-4a2a-b09c-497217c0981c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
