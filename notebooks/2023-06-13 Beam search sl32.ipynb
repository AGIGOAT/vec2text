{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a508121-bf9d-4aed-aacb-1cce9ab275b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85948908-fa3c-495d-9c14-9a60393dcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alias gtr_nq__msl32_beta__correct from /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac/checkpoint-558000\n",
      "set dataset to nq\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set num workers to 4\n",
      "Overwriting max sequence length to 32\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wy\n",
      "================ End trainer sanity check ================\n",
      "Froze 342572160 params from model type <class 'models.inversion.InversionModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The slithe and the tobogbes were mly; It wis grabbse tiring\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"gtr_nq__msl32_beta__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3df908-d91f-493e-9803-b6b3d5853f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] same rights as straight people, while 15% agreed that they should be protected from workplace discrimination. Additionally, 13% disagreed. 69% of H-\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "> \u001b[0;32m/home/jxm3/research/retrieval/inversion/trainers/base.py\u001b[0m(358)\u001b[0;36meval_generation_metrics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    356 \u001b[0;31m        num_tokens_metrics = {\n",
      "\u001b[0m\u001b[0;32m    357 \u001b[0;31m            \"pred_num_tokens\": (\n",
      "\u001b[0m\u001b[0;32m--> 358 \u001b[0;31m                \u001b[0;34m(\u001b[0m\u001b[0mpreds_sample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    359 \u001b[0;31m                \u001b[0;34m&\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    360 \u001b[0;31m                \u001b[0;34m(\u001b[0m\u001b[0mpreds_sample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_nq_loss': 0.6404249668121338, 'eval_nq_pred_num_tokens': 31.0, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.9044071345497376, 'eval_nq_token_set_recall': 0.9125329795625029, 'eval_nq_token_set_f1': 0.9080383949145814, 'eval_nq_n_ngrams_match_1': 22.024, 'eval_nq_n_ngrams_match_2': 18.236, 'eval_nq_n_ngrams_match_3': 15.748, 'eval_nq_num_true_words': 24.368, 'eval_nq_num_pred_words': 24.322, 'eval_nq_bleu_score': 75.41253923476056, 'eval_nq_meteor_score': 0.8921194996873874, 'eval_nq_rouge_score': 0.9097580497749944, 'eval_nq_exact_match': 0.414, 'eval_nq_emb_cos_sim': 0.9845502376556396, 'eval_nq_runtime': 22.2779, 'eval_nq_samples_per_second': 22.444, 'eval_nq_steps_per_second': 0.18}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a917d248-bf47-466c-ba17-e387222bb3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 13:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] the same rights as straight people, while 15% disagreed. Additionally, 13% agreed that they should be protected from workplace discrimination. 69% of H\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "{'eval_nq_loss': 0.640424907207489, 'eval_nq_pred_num_tokens': 31.0, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.9473560470950129, 'eval_nq_token_set_recall': 0.9545114912185175, 'eval_nq_token_set_f1': 0.9507149227891946, 'eval_nq_bleu_score': 82.99229769499587, 'eval_nq_meteor_score': 0.9308821960693017, 'eval_nq_rouge_score': 0.943450342989768, 'eval_nq_exact_match': 0.568, 'eval_nq_emb_cos_sim': 0.9917903542518616, 'eval_nq_runtime': 226.6175, 'eval_nq_samples_per_second': 2.206, 'eval_nq_steps_per_second': 0.141}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 16\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fde1fb-0892-4bdb-9c15-85042d884cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 50\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cb345d-feca-41e7-8b72-0548ad413879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "{'eval_nq_loss': 0.640424907207489, 'eval_nq_pred_num_tokens': 31.0, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.9318768045221499, 'eval_nq_token_set_recall': 0.94017112498065, 'eval_nq_token_set_f1': 0.9357483842436927, 'eval_nq_bleu_score': 78.53058521035312, 'eval_nq_meteor_score': 0.9108938577872909, 'eval_nq_rouge_score': 0.9278587870735759, 'eval_nq_exact_match': 0.432, 'eval_nq_emb_cos_sim': 0.9879564046859741, 'eval_nq_runtime': 115.8154, 'eval_nq_samples_per_second': 4.317, 'eval_nq_steps_per_second': 0.276}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 16\n",
    "corr_trainer.sequence_beam_width = 1\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 4,\n",
    "    \"num_return_sequences\": 4,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d75911-e544-4516-8313-d465ed5b741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a350c-e8d2-4224-9e41-01b15b1766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152bf1e-43b0-46e6-874f-4e4b304a6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 64\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 4,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4661402-72b3-436c-bda8-4232575100d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 32,\n",
    "    \"num_return_sequences\": 32,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e83f5-a932-4d23-97dc-ad79b4edccef",
   "metadata": {},
   "source": [
    "## Testing sequence-level beam width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7ce90-3c4a-4108-84c4-3673c1e283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d70a25-0a8b-416d-8cda-1d1a9eaf8ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marquette Township, Marquette County, Michigan Marquette Charter Township is a charter township of Marquette County in the U.S. state of', 'Chernenko, says that Transnistria loses $2–2.5 million daily from the Ukrainian regulations. (Transnistrian GDP is about $4']\n",
      "step 0 // scores = [-0.004439718089997768, -0.24413922429084778]\n",
      "lengths: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "step 1 // scores = [-0.05680422484874725, -0.25067004561424255]\n",
      "lengths: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "step 2 // scores = [-0.027877259999513626, -0.26591038703918457]\n",
      "lengths: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "step 3 // scores = [-0.027877259999513626, -0.24665670096874237]\n",
      "lengths: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "step 4 // scores = [-0.015157686546444893, -0.24563033878803253]\n",
      "lengths: [19, 19]\n",
      "['Marquette Charter Township, Marquette County, Michigan Marquette Charter Township is a charter town', 'In Ukrainian regulations, Chernenko says: \"Transnistria loses about $2.']\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.args.per_device_eval_batch_size = 100\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    # \"min_length\": 32, \"max_length\": 32\n",
    "}\n",
    "\n",
    "eval_batch = next(iter(corr_trainer.get_eval_dataloader(eval_dataset=corr_trainer.eval_dataset[\"nq\"])))\n",
    "one_eval_batch = {k: v[31:33] for k,v in eval_batch.items() }\n",
    "one_eval_batch = {k: v.to(corr_trainer.args.device) for k,v in one_eval_batch.items() }\n",
    "\n",
    "print(\n",
    "    corr_trainer.embedder_tokenizer.batch_decode(one_eval_batch[\"embedder_input_ids\"], skip_special_tokens=True)\n",
    ")\n",
    "\n",
    "gen_ids = corr_trainer.generate(\n",
    "    inputs=one_eval_batch,\n",
    "    generation_kwargs=corr_trainer.gen_kwargs,\n",
    "    num_recursive_steps=5,\n",
    "    sequence_beam_width=5,\n",
    ")\n",
    "print(\n",
    "    corr_trainer.embedder_tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008a875-fe28-4f48-96e0-13d8a3e2ffb8",
   "metadata": {},
   "source": [
    "## Getting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ec2256-04e2-46f9-a65e-bcc8f88a671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = corr_trainer\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def get_trajectories(inputs: Dict[str, torch.Tensor], num_beams: int = 1, num_rounds: int = 5) -> Dict:\n",
    "    gen_kwargs = copy.copy(trainer.gen_kwargs)\n",
    "    gen_kwargs[\"num_beams\"] = num_beams\n",
    "    frozen_embeddings = trainer.get_frozen_embeddings(\n",
    "        embedder_input_ids=inputs[\"embedder_input_ids\"],\n",
    "        embedder_attention_mask=inputs[\"embedder_attention_mask\"],\n",
    "    )\n",
    "    print(\"input:\", trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "    all_guesses = [trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)]\n",
    "    embeddings = []\n",
    "    for n in range(num_rounds):\n",
    "        if n == 0:\n",
    "            trainer.inversion_trainer.model.use_frozen_embeddings_as_input = False\n",
    "            hypothesis = trainer.inversion_trainer.generate(inputs=inputs, generation_kwargs=gen_kwargs)\n",
    "        else:\n",
    "            hypothesis = trainer.generate(inputs=inputs, generation_kwargs=gen_kwargs, num_recursive_steps=1)\n",
    "        # queue up next round             \n",
    "        hypothesis_embedding = trainer.embed_generated_hypothesis(input_ids=hypothesis)\n",
    "        inputs[\"frozen_embeddings\"] = frozen_embeddings\n",
    "        inputs[\"hypothesis_input_ids\"] = hypothesis\n",
    "        inputs[\"hypothesis_attention_mask\"] = (hypothesis != trainer.model.encoder_decoder.config.pad_token_id).int()\n",
    "        inputs[\"hypothesis_embedding\"] = hypothesis_embedding\n",
    "        # import pdb; pdb.set_trace()\n",
    "        embeddings.append(hypothesis_embedding.cpu())\n",
    "        dist = torch.nn.CosineSimilarity(dim=1)(hypothesis_embedding, frozen_embeddings).item()\n",
    "        print(f\"round {n+1} ({dist:.3f}):\", trainer.embedder_tokenizer.decode(hypothesis[0], skip_special_tokens=True))\n",
    "        all_guesses.append(trainer.embedder_tokenizer.decode(hypothesis[0], skip_special_tokens=True))\n",
    "    return {\n",
    "        \"true_emb\": frozen_embeddings.cpu(),\n",
    "        \"true_str\": trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True),\n",
    "        \"trajectory_emb\": torch.stack(embeddings), \n",
    "        \"trajectory_str\": all_guesses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5220e17-bc00-497d-889a-7bc012e5be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact</s>\n",
      "round 1 (0.950): to the character of the skull, which are relatively smooth and untutuous in the case of infant sutures. Unlike the sutures,\n",
      "round 2 (0.963): to the character of the skull, which are relatively smooth and untornatuous of the sutures of infant skulls. In fact, one\n",
      "round 3 (0.965): to the character of the skull, which are relatively smooth and untornatuous like those of the sutures of the infant skull. In fact\n",
      "round 4 (0.968): to the character of the sutures of the infant skull, which are relatively smooth and untornatuous. In fact, like those of one\n",
      "round 5 (0.983): to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Thus\n",
      "round 6 (0.981): to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "round 7 (0.982): to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. However\n",
      "round 8 (0.981): to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "input: individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'</s>\n",
      "round 1 (0.955): individual from the Southern Hemisphere to win the Winter Olympic relay gold medal, and was also part of the Australian Short Track team, which won gold\n",
      "round 2 (0.983): individual from the Southern Hemisphere to win a Winter Olympic gold medal, and was also part of the short track relay team that won Australia'\n",
      "round 3 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "round 4 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "round 5 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "round 6 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "round 7 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "round 8 (1.000): individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "input: the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H</s>\n",
      "round 1 (0.873): the same rights as men, and 75% agreed that they should be protected from discrimination. Among the other 15%, gay workers are no longer exp\n",
      "round 2 (0.927): the same rights as straight people, while 15% disagreed: they should be protected from discrimination and harassment. With 33% of those working people, other\n",
      "round 3 (0.965): the same rights as straight people, while 15% disagreed with discrimination. Fifteen (11%) agreed they should be protected from workplace discrimination and\n",
      "round 4 (0.981): the same rights as straight people, while 15% disagreed. 13% agreed they should be protected from workplace discrimination. Additionally, 80% disagreed H\n",
      "round 5 (0.991): the same rights as straight people, while 15% agreed that they should be protected from workplace discrimination. Additionally, 89% disagreed. 13% of H\n",
      "round 6 (0.983): same rights as straight people, while 15% agreed that they should be protected from workplace discrimination. Additionally, 13% disagreed. 69% of H-\n",
      "round 7 (0.986): the same rights as straight people. Additionally, 15% agreed that they should be protected from workplace discrimination, while 13% disagreed. 9% of H\n",
      "round 8 (0.990): the same rights as straight people. Additionally, 15% agreed that they should be protected from workplace discrimination, while 39% disagreed. 63% of H\n",
      "input: Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on</s>\n",
      "round 1 (0.954): Bruthen, Victoria Bruthen is a small town located between the River Tambo and Brudale on the Atherton In\n",
      "round 2 (0.978): Bruthen, Victoria Bruthen is a small town located along the Tambo River between Bainsdale and Ennswall\n",
      "round 3 (0.997): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Enmore on\n",
      "round 4 (1.000): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on\n",
      "round 5 (1.000): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on\n",
      "round 6 (1.000): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on\n",
      "round 7 (1.000): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on\n",
      "round 8 (1.000): Bruthen, Victoria Bruthen is a small town located alongside the Tambo River between Bairnsdale and Ensay on\n",
      "input: Castle Vale Castle Vale is a housing estate located between Erdington, Minworth and Castle Bromwich. Currently Castle Vale votes</s>\n",
      "round 1 (0.905): Castle Vale Castle Vale is a castle located between Ennisworth and Mortlake, and are currently the home of Borough Council. Els\n",
      "round 2 (0.931): Castle Vale Castle Vale is a castle located between Milneworth and Castle Bromford, both in Eddy Castle Borough. The votes\n",
      "round 3 (0.954): Castle Vale Castle Vale is a housing estate located between Erdington, Brannworth and Mischum Vale. Currently, Castle votes\n",
      "round 4 (0.980): Castle Vale Castle Vale is a housing estate located between Castle Vale, Erdington and Minnworth, Bromwich. Current votes\n",
      "round 5 (0.988): Castle Vale Castle Vale is a housing estate located between Minworth, Erdington and Bromwich. Castle Vale is currently the vote\n",
      "round 6 (0.997): Castle Vale Castle Vale is a housing estate located between Minworth, Castle Erdington and Bromwich. Currently Castle Vale votes\n",
      "round 7 (0.980): Castle Vale Castle Vale is a housing estate located between Minworth, Castle Erdington and Bromwich. Currently, Castle Vale\n",
      "round 8 (0.991): Castle Vale Castle Vale is a housing estate located between Minworth, Castle Erdington and Bromwich. Vote votes are currently\n",
      "input: 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo</s>\n",
      "round 1 (0.985): 1865. After her death, Fisk married her sister, Simona, on August 1, 1865. Deaf Smith moved to the North American\n",
      "round 2 (0.998): 1839. After her death, Fisk married her sister, Simona, on August 1, 1839. Deaf Smith moved freely between Anglo\n",
      "round 3 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "round 4 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "round 5 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "round 6 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "round 7 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "round 8 (1.000): 1839. After her death, Fisk married her sister, Simona, on August 1, 1849. Deaf Smith moved freely between Anglo\n",
      "input: in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation</s>\n",
      "round 1 (0.936): In 1978, he was a member of the Russian Composers' Society. He was commissioned by the Soviet Commission in Moscow, Broadcast\n",
      "round 2 (0.994): in 1978, he was a member of the Soviet Composers' Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "round 3 (0.983): in 1978 he was a member of the Soviet Composer's Society in Moscow. There he obtained commissions from the Russian Broadcast\n",
      "round 4 (1.000): in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "round 5 (1.000): in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "round 6 (1.000): in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "round 7 (1.000): in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "round 8 (1.000): in 1978 he was a member of the Soviet Composer's Society in Moscow. He obtained commissions from the Russian Broadcasting Corporation\n",
      "input: Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in </s>\n",
      "round 1 (0.975): Zachary Throne Zachary Throne (born September 3, 1970 in Hollywood, California) is an American actor who has appeared in television, film\n",
      "round 2 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 3 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 4 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 5 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 6 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 7 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "round 8 (1.000): Zachary Throne Zachary Throne (born April 3, 1967 in Hollywood, California) is an American actor and musician who has appeared in \n",
      "input: a design by the Quaker carpenter Owen Biddle, Jr. Biddle is best known as the author of a builder'</s>\n",
      "round 1 (0.846): the design of a Quaker biddle by the author Owen Pedrick Jones, best known as a builder and author of \"The\n",
      "round 2 (0.911): the design of a Quaker J. Owen Biddle Biddle, best known as the author of \"Builder, who was a Carpenter\n",
      "round 3 (0.930): is a design by the carpenter Owen J. Biddle, Jr. Biddle is best known as the author of \"A Builder's\n",
      "round 4 (0.966): a design by the carpenter Owen Biddle, Jr. Biddle is best known as the author of a Builder's Quaker\n",
      "round 5 (0.985): a design by the Quaker builder Owen Biddle, Jr. Biddle is best known as the author of a carpenter'\n",
      "round 6 (0.962): a design by the Quaker Carpenter Owen J. Biddle, Jr. Biddle is best known as the author of a build\n",
      "round 7 (1.000): a design by the Quaker carpenter Owen Biddle, Jr. Biddle is best known as the author of a builder'\n",
      "round 8 (1.000): a design by the Quaker carpenter Owen Biddle, Jr. Biddle is best known as the author of a builder'\n",
      "input: from Oxford in 1925. After an extra year at Oxford doing work in the Bodleian Library, in 1892 she became principal of Aberdare</s>\n",
      "round 1 (0.922): from Aberdare in the early 19th century. After two years at Aberdare she became principal of Aberdare, then head of the Oxford\n",
      "round 2 (0.961): from Oxford in 1871. While still at Oxford, before taking up the headship of the Bodleian Library she became principal of Aberdare\n",
      "round 3 (0.957): from Oxford in 1925. After spending two years at Oxford's Bodleian Library before joining the staff there in 1935 she became principal of Aberd\n",
      "round 4 (0.967): from Oxford in 1925. After one more year in the Bodleian library, working at Oxford in the 1880s she became principal of Aberare\n",
      "round 5 (0.966): from Oxford in 1925. After spending one year in the Bodleian Library, working at Oxford in the 1880s she became principal of Aberd\n",
      "round 6 (0.991): from Oxford in 1925. After working one year in the Bodleian library, then Oxford, in 1868 she became principal of Aberdare'\n",
      "round 7 (0.965): from Oxford in 1925. After working for one year in the Bodleian Library at Oxford, including being in 1876 she became principal of Aberd\n",
      "round 8 (0.988): from Oxford in 1925. After one year in the Bodleian Library, later working for Oxford, in 1867 she became principal of Aberdare\n",
      "input: was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 26 October 1803.</s>\n",
      "round 1 (0.916): was made by \"Kathi Wala\" (Vira ruler) of Puri with Clement Bardej in October 1856. In 18\n",
      "round 2 (0.955): was made by Vira Wala (Kathi Ruler) of Baroda at Jalpaiguri in October 1887.\n",
      "round 3 (0.984): was made by Vira Wala (Kathi Ruler) of Jetpur at Baroda on October 27, 1803. Colonel Walker then\n",
      "round 4 (1.000): was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 22 October 1803.\n",
      "round 5 (1.000): was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 26 October 1803.\n",
      "round 6 (1.000): was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 26 October 1803.\n",
      "round 7 (1.000): was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 26 October 1803.\n",
      "round 8 (1.000): was made by Vira Wala (Kathi Ruler) of Jetpur with Colonel Walker at Baroda on 26 October 1803.\n",
      "input: is applied to elementary variables, that is, to quantities and prices of different products and inputs. Variance accounting gives the user most possibilities for analysis.</s>\n",
      "round 1 (0.645): is applied to different variables and is therefore the most flexible of the variables. Accounting equivalence provides easy and predictable quantities to be studied by analysts\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39minversion_trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39muse_frozen_embeddings_as_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# trainer.embedder_tokenizer.decode(().flatten())\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_eval_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mget_trajectories\u001b[0;34m(inputs, num_beams, num_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39minversion_trainer\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m=\u001b[39minputs, generation_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_recursive_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# queue up next round             \u001b[39;00m\n\u001b[1;32m     24\u001b[0m hypothesis_embedding \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39membed_generated_hypothesis(input_ids\u001b[38;5;241m=\u001b[39mhypothesis)\n",
      "File \u001b[0;32m~/research/retrieval/inversion/trainers/corrector.py:245\u001b[0m, in \u001b[0;36mCorrectorTrainer.generate\u001b[0;34m(self, inputs, generation_kwargs, num_recursive_steps, sequence_beam_width)\u001b[0m\n\u001b[1;32m    242\u001b[0m num_recursive_steps_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_recursive_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 245\u001b[0m     gen_text_ids, hypothesis_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_beam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_recursive_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_recursive_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_recursive_steps_so_far\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_recursive_steps_so_far\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence_beam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_beam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gen_text_ids\n\u001b[1;32m    253\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    254\u001b[0m         gen_text_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder_decoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    255\u001b[0m     )\u001b[38;5;241m.\u001b[39mint()\n",
      "File \u001b[0;32m~/research/retrieval/inversion/trainers/corrector.py:327\u001b[0m, in \u001b[0;36mCorrectorTrainer._generate_with_beam\u001b[0;34m(self, inputs, generation_kwargs, num_recursive_steps, num_recursive_steps_so_far, sequence_beam_width)\u001b[0m\n\u001b[1;32m    325\u001b[0m     gen_text_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((bos_token_ids, gen_text_ids[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_corrector_encoder:\n\u001b[0;32m--> 327\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     gen_text_ids \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msequences\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# get scores for sequences\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075\u001b[39;00m\n",
      "File \u001b[0;32m~/research/retrieval/inversion/models/corrector_encoder.py:147\u001b[0m, in \u001b[0;36mCorrectorEncoderModel.generate\u001b[0;34m(self, inputs, generation_kwargs, return_dict_in_generate)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_decoder\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;66;03m# required: input embeddings\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_kwargs,\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# required: input embeddings\u001b[39;49;00m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# optional: input IDs (for starting generation).\u001b[39;49;00m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# typically not set unless generating prefixes for\u001b[39;49;00m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# reranking.\u001b[39;49;00m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1515\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1510\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m         )\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:2332\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2329\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2332\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1716\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:753\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    750\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:342\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:254\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 254\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corr_trainer.args.per_device_eval_batch_size = 100\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"min_length\": 32, \"max_length\": 32,\n",
    "}\n",
    "\n",
    "eval_batch = next(iter(trainer.get_eval_dataloader(eval_dataset=trainer.eval_dataset[\"nq\"])))\n",
    "\n",
    "data = []\n",
    "for i in range(len(eval_batch[\"input_ids\"])):\n",
    "    one_eval_batch = {k: v[None, i] for k,v in eval_batch.items() }\n",
    "    one_eval_batch = {k: v.to(trainer.args.device) for k,v in one_eval_batch.items() } # put on GPU\n",
    "    trainer.inversion_trainer.model.use_frozen_embeddings_as_input = False\n",
    "    # trainer.embedder_tokenizer.decode(().flatten())\n",
    "    data.append(get_trajectories(inputs=one_eval_batch, num_rounds=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba15d95-041f-4460-9097-a10eb8266ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data, open(\"trajectories_sl32.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0916d4-1fc8-4c4e-9f37-3ee3a3752866",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).iloc[26][\"trajectory_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231aba86-dea8-401a-a65c-79315987239d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
