{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a508121-bf9d-4aed-aacb-1cce9ab275b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85948908-fa3c-495d-9c14-9a60393dcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alias gtr_nq__msl32_beta__correct from /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac/checkpoint-558000\n",
      "set dataset to nq\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming keys {'embedding_transform.2.bias', 'embedding_transform.2.weight'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wre\n",
      "================ End trainer sanity check ================\n",
      "Froze 342572160 params from model type <class 'models.inversion.InversionModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jxm3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming keys {'embedding_transform.2.bias', 'embedding_transform.2.weight'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The slithe and the tobogbes were mly; It wis grabbse tiring\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"gtr_nq__msl32_beta__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3df908-d91f-493e-9803-b6b3d5853f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917d248-bf47-466c-ba17-e387222bb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 16\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cb345d-feca-41e7-8b72-0548ad413879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] the same rights as straight people, while 15% disagreed. Additionally, 13% agreed that they should be protected from workplace discrimination. 69% of H\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "{'eval_nq_loss': 0.6404249668121338, 'eval_nq_pred_num_tokens': 31.0, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.9623660071173297, 'eval_nq_token_set_recall': 0.9669919096435383, 'eval_nq_token_set_f1': 0.9645378046283803, 'eval_nq_bleu_score': 87.39311717762428, 'eval_nq_meteor_score': 0.9510865378881334, 'eval_nq_rouge_score': 0.9606096618268278, 'eval_nq_exact_match': 0.652, 'eval_nq_emb_cos_sim': 0.9934468269348145, 'eval_nq_runtime': 458.6433, 'eval_nq_samples_per_second': 1.09, 'eval_nq_steps_per_second': 0.137}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 8\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d75911-e544-4516-8313-d465ed5b741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.sequence_beam_width = 4\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a350c-e8d2-4224-9e41-01b15b1766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152bf1e-43b0-46e6-874f-4e4b304a6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 64\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 4,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4661402-72b3-436c-bda8-4232575100d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.return_best_hypothesis = True\n",
    "corr_trainer.args.per_device_eval_batch_size = 8\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 32,\n",
    "    \"num_return_sequences\": 32,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 5\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e83f5-a932-4d23-97dc-ad79b4edccef",
   "metadata": {},
   "source": [
    "## Testing sequence-level beam width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d70a25-0a8b-416d-8cda-1d1a9eaf8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.args.per_device_eval_batch_size = 100\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"min_length\": 32, \"max_length\": 32\n",
    "}\n",
    "\n",
    "eval_batch = next(iter(corr_trainer.get_eval_dataloader(eval_dataset=corr_trainer.eval_dataset[\"nq\"])))\n",
    "one_eval_batch = {k: v[31:33] for k,v in eval_batch.items() }\n",
    "one_eval_batch = {k: v.to(corr_trainer.args.device) for k,v in one_eval_batch.items() }\n",
    "\n",
    "print(\n",
    "    corr_trainer.embedder_tokenizer.batch_decode(one_eval_batch[\"embedder_input_ids\"], skip_special_tokens=True)\n",
    ")\n",
    "\n",
    "gen_ids = corr_trainer.generate(\n",
    "    inputs=one_eval_batch,\n",
    "    generation_kwargs=corr_trainer.gen_kwargs,\n",
    "    num_recursive_steps=5,\n",
    "    sequence_beam_width=5,\n",
    ")\n",
    "print(\n",
    "    corr_trainer.embedder_tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008a875-fe28-4f48-96e0-13d8a3e2ffb8",
   "metadata": {},
   "source": [
    "## Getting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec2256-04e2-46f9-a65e-bcc8f88a671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = corr_trainer\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def get_trajectories(inputs: Dict[str, torch.Tensor], num_beams: int = 1, num_rounds: int = 5) -> Dict:\n",
    "    gen_kwargs = copy.copy(trainer.gen_kwargs)\n",
    "    gen_kwargs[\"num_beams\"] = num_beams\n",
    "    frozen_embeddings = trainer.get_frozen_embeddings(\n",
    "        embedder_input_ids=inputs[\"embedder_input_ids\"],\n",
    "        embedder_attention_mask=inputs[\"embedder_attention_mask\"],\n",
    "    )\n",
    "    print(\"input:\", trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "    all_guesses = [trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)]\n",
    "    embeddings = []\n",
    "    for n in range(num_rounds):\n",
    "        if n == 0:\n",
    "            trainer.inversion_trainer.model.use_frozen_embeddings_as_input = False\n",
    "            hypothesis = trainer.inversion_trainer.generate(inputs=inputs, generation_kwargs=gen_kwargs)\n",
    "        else:\n",
    "            hypothesis = trainer.generate(inputs=inputs, generation_kwargs=gen_kwargs, num_recursive_steps=1)\n",
    "        # queue up next round             \n",
    "        hypothesis_embedding = trainer.embed_generated_hypothesis(input_ids=hypothesis)\n",
    "        inputs[\"frozen_embeddings\"] = frozen_embeddings\n",
    "        inputs[\"hypothesis_input_ids\"] = hypothesis\n",
    "        inputs[\"hypothesis_attention_mask\"] = (hypothesis != trainer.model.encoder_decoder.config.pad_token_id).int()\n",
    "        inputs[\"hypothesis_embedding\"] = hypothesis_embedding\n",
    "        # import pdb; pdb.set_trace()\n",
    "        embeddings.append(hypothesis_embedding.cpu())\n",
    "        dist = torch.nn.CosineSimilarity(dim=1)(hypothesis_embedding, frozen_embeddings).item()\n",
    "        print(f\"round {n+1} ({dist:.3f}):\", trainer.embedder_tokenizer.decode(hypothesis[0], skip_special_tokens=True))\n",
    "        all_guesses.append(trainer.embedder_tokenizer.decode(hypothesis[0], skip_special_tokens=True))\n",
    "    return {\n",
    "        \"true_emb\": frozen_embeddings.cpu(),\n",
    "        \"true_str\": trainer.embedder_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True),\n",
    "        \"trajectory_emb\": torch.stack(embeddings), \n",
    "        \"trajectory_str\": all_guesses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5220e17-bc00-497d-889a-7bc012e5be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.args.per_device_eval_batch_size = 100\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "\n",
    "eval_batch = next(iter(trainer.get_eval_dataloader(eval_dataset=trainer.eval_dataset[\"nq\"])))\n",
    "\n",
    "data = []\n",
    "for i in range(len(eval_batch[\"input_ids\"])):\n",
    "    one_eval_batch = {k: v[None, i] for k,v in eval_batch.items() }\n",
    "    one_eval_batch = {k: v.to(trainer.args.device) for k,v in one_eval_batch.items() } # put on GPU\n",
    "    trainer.inversion_trainer.model.use_frozen_embeddings_as_input = False\n",
    "    # trainer.embedder_tokenizer.decode(().flatten())\n",
    "    data.append(get_trajectories(inputs=one_eval_batch, num_rounds=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba15d95-041f-4460-9097-a10eb8266ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data, open(\"trajectories_sl32.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0916d4-1fc8-4c4e-9f37-3ee3a3752866",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).iloc[26][\"trajectory_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231aba86-dea8-401a-a65c-79315987239d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
