{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558b9f14-957a-4a73-89cc-bd4cef344872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3920557d-58c2-4690-abc5-690b6769f582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-28 13:24:26,854] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/jxm3/.conda/envs/torch/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "loading alias gtr_nq__msl32_beta__correct from /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac/checkpoint-558000\n",
      "set dataset to nq\n",
      "Corrector encoder noise level 1e-05\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set num workers to 4\n",
      "Overwriting max sequence length from 32 to 32\n",
      "Overwriting use_less_data from None to -1\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = False\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wy\n",
      "================ End trainer sanity check ================\n",
      "Froze 342572160 params from model type <class 'models.inversion.InversionModel'>\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The slithe and the tobogbes were mly; It wis grabbse tiring\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"gtr_nq__msl32_beta__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e49f54b-059e-4721-85cb-1a1d75816e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alias clinicalbert_nq__msl32 from /home/jxm3/research/retrieval/inversion/saves/01c63decd9009f5961504b52a96cd324...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/01c63decd9009f5961504b52a96cd324/checkpoint-249500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = False\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> and Ivory, and the Mobs, were Gwendolyn the Barber, Ally Griffiths to Mobse\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "cb_experiment, cb_trainer = aliases.load_experiment_and_trainer_from_alias(\"clinicalbert_nq__msl32\")\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef341fc-0a47-4ddb-bf45-f77d451a70ac",
   "metadata": {},
   "source": [
    "## switch embedders and measure perf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbad9e7d-cd5a-44d6-ae64-807040d88f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_trainer.inversion_trainer.model.embedder = cb_trainer.model.embedder\n",
    "corr_trainer.inversion_trainer.embedder_tokenizer = cb_trainer.embedder_tokenizer\n",
    "corr_trainer.embedder_tokenizer = cb_trainer.embedder_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad742202-ba2d-4f04-a8b7-3aa75d5a83ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000801ff62e00000653'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000000011f62300000652'\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000010026201d00000654'\n",
      "generating from val:  67%|██████████████████████████████████████████████████████████                             | 2/3 [00:01<00:00,  1.76it/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000020012314a00000657'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000002801a363400000655'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000001801db8b100000656'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred]  ''' ''' ''  '''   ''' and and that that '''\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] ( ( ( ( ( ( ( ( ( ( ( ( ()))) or or or or or or or or or or or other other other\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] of a lane of the road, and those of other vehicles, including National Guard and World Trade Network's disability scooter and leg spanner documentation\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 12.425542831420898,\n",
       " 'eval_accuracy': 0.06888440860215053,\n",
       " 'eval_pred_num_tokens': 31.0,\n",
       " 'eval_true_num_tokens': 32.0,\n",
       " 'eval_token_set_precision': 0.049809417714041355,\n",
       " 'eval_token_set_recall': 0.14835501600502735,\n",
       " 'eval_token_set_f1': 0.06401793867180895,\n",
       " 'eval_token_set_f1_sem': 0.0031835246945319763,\n",
       " 'eval_n_ngrams_match_1': 1.2994791666666667,\n",
       " 'eval_n_ngrams_match_2': 0.03125,\n",
       " 'eval_n_ngrams_match_3': 0.0,\n",
       " 'eval_num_true_words': 24.231770833333332,\n",
       " 'eval_num_pred_words': 21.645833333333332,\n",
       " 'eval_bleu_score': 1.0358018797464714,\n",
       " 'eval_bleu_score_sem': 0.04863709414452401,\n",
       " 'eval_rouge_score': 0.041604617943008196,\n",
       " 'eval_exact_match': 0.0,\n",
       " 'eval_exact_match_sem': 0.0,\n",
       " 'eval_emb_cos_sim': 0.4859704375267029,\n",
       " 'eval_emb_cos_sim_sem': 0.00799008460511141,\n",
       " 'eval_perplexity': 249083.34328242522,\n",
       " 'eval_runtime': 5.3901,\n",
       " 'eval_samples_per_second': 92.763,\n",
       " 'eval_steps_per_second': 0.742}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = corr_trainer.inversion_trainer.evaluate(\n",
    "    eval_dataset=cb_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval\",\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ceb8fc5-9f71-452f-b721-9297c7c72350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 04:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000007007e18a100000647'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000068010506100000646'\n",
      "generating from val:  50%|███████████████████████████████████████████▌                                           | 1/2 [00:01<00:01,  1.71s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000008007dd80200000648'\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000007800ecd5700000649'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] 's's's's's'and that there will be a \"by that, and that overall\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] Oral history: or, or otherwise, a. Or, or other other than other other results: There is an overall. Find out\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] of the World Trade Network, and others, including a scottish ring of 3: National Guard. Toge and first legionary\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.894082069396973,\n",
       " 'eval_pred_num_tokens': 31.0,\n",
       " 'eval_true_num_tokens': 32.0,\n",
       " 'eval_token_set_precision': 0.10589851647901183,\n",
       " 'eval_token_set_recall': 0.15522923339130806,\n",
       " 'eval_token_set_f1': 0.122817531684973,\n",
       " 'eval_token_set_f1_sem': 0.0031683070923323284,\n",
       " 'eval_n_ngrams_match_1': 2.572,\n",
       " 'eval_n_ngrams_match_2': 0.098,\n",
       " 'eval_n_ngrams_match_3': 0.0,\n",
       " 'eval_num_true_words': 24.368,\n",
       " 'eval_num_pred_words': 21.296,\n",
       " 'eval_bleu_score': 1.793210525343891,\n",
       " 'eval_bleu_score_sem': 0.039200822844922406,\n",
       " 'eval_rouge_score': 0.0755257244072943,\n",
       " 'eval_exact_match': 0.0,\n",
       " 'eval_exact_match_sem': 0.0,\n",
       " 'eval_emb_cos_sim': 0.6202307343482971,\n",
       " 'eval_emb_cos_sim_sem': 0.004141423933008237,\n",
       " 'eval_runtime': 10.2579,\n",
       " 'eval_samples_per_second': 48.743,\n",
       " 'eval_steps_per_second': 0.195}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=cb_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval\",\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2f7f9f-7686-4f9f-a44d-f90874db5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000f00199d020000065b'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000000011f62400000659'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000001000d355af0000065a'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000f8039541e00000658'\n",
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:22<00:07,  7.48s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000500058e0c0000065e'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000003800aedd70000065d'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000400202c0b0000065c'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000004800ad8780000065f'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] Conta con overall There is currently a \"conta con overall\": Mykon mykon is a to begin with and to \n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] Oral history: There is a find out result: Oral history: There is an other than overall. There is an overall's\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] Toge of the World ring of 3 Toge of the World ring of 3: scottish including first, including first, name \"\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.899476051330566,\n",
       " 'eval_pred_num_tokens': 31.0,\n",
       " 'eval_true_num_tokens': 32.0,\n",
       " 'eval_token_set_precision': 0.09765917152575458,\n",
       " 'eval_token_set_recall': 0.1492982927375341,\n",
       " 'eval_token_set_f1': 0.11568393713600876,\n",
       " 'eval_token_set_f1_sem': 0.005885589980298532,\n",
       " 'eval_n_ngrams_match_1': 2.33,\n",
       " 'eval_n_ngrams_match_2': 0.08,\n",
       " 'eval_n_ngrams_match_3': 0.0,\n",
       " 'eval_num_true_words': 24.54,\n",
       " 'eval_num_pred_words': 22.03,\n",
       " 'eval_bleu_score': 1.7296105756204019,\n",
       " 'eval_bleu_score_sem': 0.07696850228720942,\n",
       " 'eval_rouge_score': 0.06536071021754233,\n",
       " 'eval_exact_match': 0.0,\n",
       " 'eval_exact_match_sem': 0.0,\n",
       " 'eval_emb_cos_sim': 0.6546975374221802,\n",
       " 'eval_emb_cos_sim_sem': 0.007200171798467636,\n",
       " 'eval_runtime': 33.1037,\n",
       " 'eval_samples_per_second': 3.021,\n",
       " 'eval_steps_per_second': 0.121}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 32\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 20\n",
    "corr_trainer.sequence_beam_width = 1\n",
    "corr_trainer.return_best_hypothesis = False\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=cb_trainer.eval_dataset[\"nq\"].select(range(100)),\n",
    "    metric_key_prefix=\"eval\",\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa5b212-4859-4d51-b25f-ae7d822c7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000500058e0d00000662'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000060008de3500000660'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000004800ad87900000661'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000580092e0100000663'\n",
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:53<00:17, 17.81s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000007800ecd5b00000664'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000068010506400000667'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000007007e18b100000666'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000008007dd80600000665'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] Conta con overall There is currently a \"conta con overall\": Mykon mykon is a to begin with and to \n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] Oral history: There is a find out result: Oral history: There is an other than overall. There is an overall's\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] Toge of the World ring of 3 Toge of the World ring of 3: scottish including first, including first, name \"\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.899476051330566,\n",
       " 'eval_pred_num_tokens': 31.0,\n",
       " 'eval_true_num_tokens': 32.0,\n",
       " 'eval_token_set_precision': 0.09765917152575458,\n",
       " 'eval_token_set_recall': 0.1492982927375341,\n",
       " 'eval_token_set_f1': 0.11568393713600876,\n",
       " 'eval_token_set_f1_sem': 0.005885589980298532,\n",
       " 'eval_n_ngrams_match_1': 2.33,\n",
       " 'eval_n_ngrams_match_2': 0.08,\n",
       " 'eval_n_ngrams_match_3': 0.0,\n",
       " 'eval_num_true_words': 24.54,\n",
       " 'eval_num_pred_words': 22.03,\n",
       " 'eval_bleu_score': 1.7296105756204019,\n",
       " 'eval_bleu_score_sem': 0.07696850228720942,\n",
       " 'eval_rouge_score': 0.06536071021754233,\n",
       " 'eval_exact_match': 0.0,\n",
       " 'eval_exact_match_sem': 0.0,\n",
       " 'eval_emb_cos_sim': 0.6546975374221802,\n",
       " 'eval_emb_cos_sim_sem': 0.007200171798467636,\n",
       " 'eval_runtime': 73.7719,\n",
       " 'eval_samples_per_second': 1.356,\n",
       " 'eval_steps_per_second': 0.054}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 32\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 50\n",
    "corr_trainer.sequence_beam_width = 1\n",
    "corr_trainer.return_best_hypothesis = False\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=cb_trainer.eval_dataset[\"nq\"].select(range(100)),\n",
    "    metric_key_prefix=\"eval\",\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044e535-dfe2-4c9a-948b-e187d645e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000c800cf60a0000066a'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000d000b7a0d00000668'\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000d800a2e720000066b'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000e000b460e00000669'\n",
      "generating from val:  88%|██████████████████████████████████████████████████████████████████████████▊          | 22/25 [23:21<03:09, 63.10s/it]"
     ]
    }
   ],
   "source": [
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 4\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 50\n",
    "corr_trainer.sequence_beam_width = 8\n",
    "corr_trainer.return_best_hypothesis = False\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=cb_trainer.eval_dataset[\"nq\"].select(range(100)),\n",
    "    metric_key_prefix=\"eval\",\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58851046-6463-4c49-b19e-b62f9c5ed8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
