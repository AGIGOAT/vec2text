{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d454e5f-97e2-40c0-bd56-a206ec6cdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb3720b-118a-4902-959c-646ca6bcc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-28 14:01:06,306] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/jxm3/.conda/envs/torch/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "loading alias gtr_nq__msl32_beta__correct from /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac/checkpoint-558000\n",
      "set dataset to nq\n",
      "Corrector encoder noise level 1e-05\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set num workers to 4\n",
      "Overwriting max sequence length from 32 to 32\n",
      "Overwriting use_less_data from None to -1\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = False\n",
      "Renaming keys {'embedding_transform.2.bias', 'embedding_transform.2.weight'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wy\n",
      "================ End trainer sanity check ================\n",
      "Froze 342572160 params from model type <class 'models.inversion.InversionModel'>\n",
      "Renaming keys {'embedding_transform.2.bias', 'embedding_transform.2.weight'} for backward compatibility.\n",
      "Renaming keys {'embedding_transform.2.bias', 'embedding_transform.2.weight'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The slithe and the tobogbes were mly; It wis grabbse tiring\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"gtr_nq__msl32_beta__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93125837-3351-4f67-b89e-0f2227e459c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 56:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000007800ecd5e00000672'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000009000b860900000673'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000008007dd80f00000670'\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000088008f7a200000671'\n",
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:03<00:01,  1.15s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000a0010822200000677'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000a801b540e00000674'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000b000db93500000676'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000009800e3cc000000675'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] —————————————but in reality, \"My first steps.\" The \"My first steps\" would indicate\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] () / / / / / / / / In the process, a'shan't \n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] of the ribs, fiber fiber fiber, and tendons of the lungs of the two people who lunge. The lungs of the skip\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:09<00:03,  3.28s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000d000b7c0000000678'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000b800aec280000067b'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000c800cf6120000067a'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000c000f77e900000679'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] s s s The canon s s s s sa is a canon of a canon until\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] Laton Story Laton Story is First Story where Laton Blow is Second Story and Hebbi Blue Letter. To obtain \n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] when  can officially be, when after  can officially be. When  can officially be, the only \n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "{'eval_nq_loss': 9.321857452392578, 'eval_nq_pred_num_tokens': 31.0, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.08513926302005163, 'eval_nq_token_set_recall': 0.1616869419952779, 'eval_nq_token_set_f1': 0.10610246396859119, 'eval_nq_token_set_f1_sem': 0.00319242383349615, 'eval_nq_n_ngrams_match_1': 2.018, 'eval_nq_n_ngrams_match_2': 0.056, 'eval_nq_n_ngrams_match_3': 0.002, 'eval_nq_num_true_words': 24.368, 'eval_nq_num_pred_words': 21.402, 'eval_nq_bleu_score': 1.4777216325102027, 'eval_nq_bleu_score_sem': 0.03799587418427152, 'eval_nq_rouge_score': 0.0652110813127208, 'eval_nq_exact_match': 0.0, 'eval_nq_exact_match_sem': 0.0, 'eval_nq_emb_cos_sim': 0.018419606611132622, 'eval_nq_emb_cos_sim_sem': 0.0016323087745785815, 'eval_nq_5round_pred_num_tokens': 31.0, 'eval_nq_5round_true_num_tokens': 32.0, 'eval_nq_5round_token_set_precision': 0.09337152267200244, 'eval_nq_5round_token_set_recall': 0.15254805930253804, 'eval_nq_5round_token_set_f1': 0.1121468536529818, 'eval_nq_5round_token_set_f1_sem': 0.0032488334356773796, 'eval_nq_5round_n_ngrams_match_1': 2.256, 'eval_nq_5round_n_ngrams_match_2': 0.074, 'eval_nq_5round_n_ngrams_match_3': 0.0, 'eval_nq_5round_num_true_words': 24.368, 'eval_nq_5round_num_pred_words': 22.068, 'eval_nq_5round_bleu_score': 1.6573177706210351, 'eval_nq_5round_bleu_score_sem': 0.03664749288410949, 'eval_nq_5round_rouge_score': 0.0678444917118402, 'eval_nq_5round_exact_match': 0.0, 'eval_nq_5round_exact_match_sem': 0.0, 'eval_nq_5round_emb_cos_sim': 0.019312405958771706, 'eval_nq_5round_emb_cos_sim_sem': 0.001676241373537094, 'eval_nq_runtime': 28.809, 'eval_nq_samples_per_second': 17.356, 'eval_nq_steps_per_second': 0.139}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.inversion_trainer.model.noise_level = 1e-1\n",
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 1\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "320b0bc9-d799-4274-a95b-1246a5c38ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000e802464e6000006f4'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000f803a7584000006f3'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000e000b5a56000006f2'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000f00199d1a000006f1'\n",
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:03<00:01,  1.19s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000001000d355d2000006f6'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000080200005000006f8'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000100262c35000006f5'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000000120251000006f7'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] Dear people at the aforementioned show: New mediating local show: Leag. With New Zealand show first, under foster Le Le Andrew Bravo\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] In \"tired by no other paper, there will be an unsuccessful opportunity to hold. Hello –info titlus Hello –minte make true\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] first in the cast. In the process, local and cast Cone cast making it first. Torii cast first, \"something joint first miscare Boy\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating from val:  75%|█████████████████████████████████████████████████████████████████▎                     | 3/4 [00:10<00:03,  3.26s/it]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/home/jxm3/.conda/envs/torch/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000002801a3c1a000006fb'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000001801db8d9000006fc'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000030009fc33000006f9'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000200123166000006fa'\n",
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pred] to the character of the sutures of the skull, which, like those of the infant skull, are relatively smooth and untornuous. Indeed\n",
      "[true] to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact\n",
      "\n",
      "\n",
      "\n",
      "[pred] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "[true] individual from the Southern Hemisphere, to win a Winter Olympic gold medal and was also part of the short track relay team that won Australia'\n",
      "\n",
      "\n",
      "\n",
      "[pred] same rights as straight people, while 15% agreed that they should be protected from workplace discrimination. Additionally, 13% disagreed. 69% of H-\n",
      "[true] the same rights as straight people, while 15% disagreed. Additionally, 69% agreed that they should be protected from workplace discrimination. 13% of H\n",
      "{'eval_nq_loss': 9.122942924499512, 'eval_nq_pred_num_tokens': 30.703125, 'eval_nq_true_num_tokens': 32.0, 'eval_nq_token_set_precision': 0.1310052081220001, 'eval_nq_token_set_recall': 0.14084919580952496, 'eval_nq_token_set_f1': 0.13341118446060574, 'eval_nq_token_set_f1_sem': 0.0029473218287045916, 'eval_nq_n_ngrams_match_1': 3.008, 'eval_nq_n_ngrams_match_2': 0.108, 'eval_nq_n_ngrams_match_3': 0.004, 'eval_nq_num_true_words': 24.368, 'eval_nq_num_pred_words': 24.356, 'eval_nq_bleu_score': 1.942820257157332, 'eval_nq_bleu_score_sem': 0.03295124025342177, 'eval_nq_rouge_score': 0.0882117786372319, 'eval_nq_exact_match': 0.0, 'eval_nq_exact_match_sem': 0.0, 'eval_nq_emb_cos_sim': 0.13729210197925568, 'eval_nq_emb_cos_sim_sem': 0.002488990372568428, 'eval_nq_5round_pred_num_tokens': 31.0, 'eval_nq_5round_true_num_tokens': 32.0, 'eval_nq_5round_token_set_precision': 0.9044071345497376, 'eval_nq_5round_token_set_recall': 0.9125329795625029, 'eval_nq_5round_token_set_f1': 0.9080383949145814, 'eval_nq_5round_token_set_f1_sem': 0.005791854183005015, 'eval_nq_5round_n_ngrams_match_1': 22.024, 'eval_nq_5round_n_ngrams_match_2': 18.236, 'eval_nq_5round_n_ngrams_match_3': 15.748, 'eval_nq_5round_num_true_words': 24.368, 'eval_nq_5round_num_pred_words': 24.322, 'eval_nq_5round_bleu_score': 73.70467302564771, 'eval_nq_5round_bleu_score_sem': 1.286482179234871, 'eval_nq_5round_rouge_score': 0.9097580497749944, 'eval_nq_5round_exact_match': 0.414, 'eval_nq_5round_exact_match_sem': 0.02204949796982786, 'eval_nq_5round_emb_cos_sim': 0.9806452989578247, 'eval_nq_5round_emb_cos_sim_sem': 0.0013347002818594668, 'eval_nq_runtime': 28.5825, 'eval_nq_samples_per_second': 17.493, 'eval_nq_steps_per_second': 0.14}\n"
     ]
    }
   ],
   "source": [
    "corr_trainer.inversion_trainer.model._stored_embeddings = {}\n",
    "corr_trainer.inversion_trainer.model.noise_level = 1\n",
    "corr_trainer.return_best_hypothesis = False\n",
    "corr_trainer.args.per_device_eval_batch_size = 128\n",
    "corr_trainer.sequence_beam_width = 1\n",
    "\n",
    "corr_trainer.gen_kwargs = {\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "}\n",
    "corr_trainer.num_gen_recursive_steps = 1\n",
    "metrics = corr_trainer.evaluate(\n",
    "    eval_dataset=corr_trainer.eval_dataset[\"nq\"].select(range(500)),\n",
    "    metric_key_prefix=\"eval_nq\",\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde88070-97ed-491b-a7b2-845cde184a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corr_trainer.inversion_trainer.model._stored_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f95b843a-121f-48ff-ba09-85252a0538a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_embeddings = {}\n",
    "for k,v in corr_trainer.inversion_trainer.model._stored_embeddings.items():\n",
    "    s = corr_trainer.embedder_tokenizer.decode(\n",
    "        k,\n",
    "        # skip_special_tokens=True\n",
    "    )\n",
    "    noisy_embeddings[s] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b938f86a-bf40-4a95-a98b-4dcbd84e90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_embeddings = corr_trainer.preds_emb\n",
    "noisy_embeddings = corr_trainer.inversion_trainer.model._stored_embeddings\n",
    "true_embeddings = corr_trainer.labels_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8539f531-2017-4290-a6f1-0e2e123ffaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_input_ids = torch.tensor(corr_trainer.preds_sample_labels_list).to(corr_trainer.args.device) # eval set\n",
    "true_attention_mask = (true_input_ids != pad_token_id).to(corr_trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a89612c-6c53-4da9-bd0a-ca05210b3f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to the character of the sutures of the skull which, like those of the infant skull, are relatively smooth and untortuous. In fact</s>'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_trainer.embedder_tokenizer.decode(true_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbf9ebca-9248-4674-a891-754202182884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'do_sample': False,\n",
       " 'no_repeat_ngram_size': 0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_trainer.gen_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e744d462-0e52-4f55-9387-7a07e875ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "corr_trainer.inversion_trainer.model.noise_level = 0\n",
    "pad_token_id = corr_trainer.pad_token_id\n",
    "\n",
    "\n",
    "corr_trainer.inversion_trainer.model.noise_level = 1e-1\n",
    "noisy_embeddings = corr_trainer.inversion_trainer.call_embedding_model(\n",
    "    input_ids=true_input_ids, \n",
    "    attention_mask=true_attention_mask\n",
    ")\n",
    "corr_trainer.inversion_trainer.model.noise_level = 0.0\n",
    "\n",
    "fake_embedder_input_ids = torch.ones_like(true_input_ids, device=true_input_ids.device)\n",
    "fake_embedder_attention_mask = torch.ones_like(true_attention_mask, device=true_attention_mask.device)\n",
    "\n",
    "corr_trainer.inversion_trainer.model.use_frozen_embeddings_as_input = True\n",
    "preds_sample = corr_trainer.generate(\n",
    "    inputs={ \"frozen_embeddings\": noisy_embeddings, \"embedder_input_ids\": fake_embedder_input_ids },\n",
    "    generation_kwargs = {'early_stopping': False,  'num_beams': 1,  'do_sample': False,  'no_repeat_ngram_size': 0, 'max_length': 32 },\n",
    ")\n",
    "\n",
    "eos_token_id = corr_trainer.embedder_tokenizer.eos_token_id\n",
    "eos_tokens = (\n",
    "    torch.ones(\n",
    "        (len(preds_sample), 1),\n",
    "        dtype=torch.long,\n",
    "        device=corr_trainer.args.device,\n",
    "    )\n",
    "    * eos_token_id\n",
    ")\n",
    "preds_sample = torch.cat((preds_sample[:, 1:], eos_tokens), dim=1)\n",
    "pred_text_embeddings = corr_trainer.call_embedding_model(\n",
    "    input_ids=preds_sample,\n",
    "    attention_mask=(preds_sample != pad_token_id).to(\n",
    "        corr_trainer.args.device\n",
    "    ),\n",
    ")\n",
    "true_embeddings = corr_trainer.inversion_trainer.call_embedding_model(\n",
    "    input_ids=true_input_ids, \n",
    "    attention_mask=true_attention_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1e4a29dc-5991-4671-8e9f-54c8052b9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS = torch.nn.functional.cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "04df795e-5297-44c0-a089-66c77dfe39b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0020, device='cuda:0')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((CS(true_embeddings, pred_text_embeddings) < CS(noisy_embeddings, pred_text_embeddings))).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dedf010d-7309-4028-a05f-2762be7eb73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1592, device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CS(true_embeddings, pred_text_embeddings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c4b966cb-1933-42bf-a15e-7434ddabd491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0840, device='cuda:0')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CS(noisy_embeddings, pred_text_embeddings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267824c6-7106-49e4-95ee-1521b41bd7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
