{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655bfccb-8d8d-4619-9da0-b66c1477a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import sys\n",
    "sys.path = [\n",
    "    p for p in sys.path\n",
    "    if p not in ['/home/jxm3/research/prompting/imodelsX', '/home/jxm3/research/prompting/tree-prompt']\n",
    "]\n",
    "sys.path.append('/home/jxm3/research/retrieval/inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b84be31-ba4c-4740-bdbd-930a43d5b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-29 09:47:58,802] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/jxm3/.conda/envs/torch/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "loading alias gtr_nq__msl32_beta__correct from /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac...\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/47d9c149a8e827d0609abbeefdfd89ac/checkpoint-558000\n",
      "set dataset to nq\n",
      "Corrector encoder noise level 1e-05\n",
      "loading alias dpr_nq__msl32_beta from /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea...\n",
      "Set num workers to 4\n",
      "Overwriting max sequence length from 32 to 32\n",
      "Overwriting use_less_data from None to -1\n",
      "> checkpoint: /home/jxm3/research/retrieval/inversion/saves/db66b9c01b644541fedbdcc59c53a285/ebb31d91810c4b62d2b55b5382e8c7ea/checkpoint-999744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with TOKENIZERS_PARALLELISM = False\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The mlbies wase wyst bograge; And the sliths and toms wy\n",
      "================ End trainer sanity check ================\n",
      "Froze 342572160 params from model type <class 'models.inversion.InversionModel'>\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "Renaming keys {'embedding_transform.2.weight', 'embedding_transform.2.bias'} for backward compatibility.\n",
      "================ Begin trainer sanity check ================\n",
      "\tInput to encode -> Twas brillig, and the slithy toves, Did gyre and gimble in the wabe, All mimsy were the borogoves, And the mome raths outgrabe.\n",
      "\tDecoded output shape ->  torch.Size([1, 33])\n",
      "\tDecoded output -> The slithe and the tobogbes were mly; It wis grabbse tiring\n",
      "================ End trainer sanity check ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import aliases\n",
    "\n",
    "# inv_trainer = aliases.load_trainer_from_alias(\"openai_msmarco__msl128__100epoch\")\n",
    "corr_experiment, corr_trainer = aliases.load_experiment_and_trainer_from_alias(\"gtr_nq__msl32_beta__correct\")\n",
    "inv_trainer = corr_trainer.inversion_trainer\n",
    "# corr_trainer.precompute_hypotheses()\n",
    "corr_trainer.model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e56d035-700f-4c36-91c4-aac911a1c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11496873  0.06054432 -0.01189671 ... -0.23369355  0.03117392\n",
      "   0.6132764 ]\n",
      " [-0.0163618   0.29337484  0.21915755 ... -0.42278576  0.05200446\n",
      "   0.51192284]]\n"
     ]
    }
   ],
   "source": [
    "# load DPR\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "dpr = SentenceTransformer('sentence-transformers/facebook-dpr-ctx_encoder-multiset-base')\n",
    "embeddings = dpr.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ab9483d-7162-4fa0-aeb3-4a79f65c6fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BEIR dataset: arguana\n",
      "loading BEIR dataset: climate-fever\n",
      "loading BEIR dataset: cqadupstack\n",
      "loading BEIR dataset: dbpedia-entity\n",
      "loading BEIR dataset: fever\n",
      "loading BEIR dataset: fiqa\n",
      "loading BEIR dataset: hotpotqa\n",
      "loading BEIR dataset: msmarco\n",
      "loading BEIR dataset: nfcorpus\n",
      "loading BEIR dataset: nq\n",
      "loading BEIR dataset: quora\n",
      "loading BEIR dataset: scidocs\n",
      "loading BEIR dataset: scifact\n",
      "loading BEIR dataset: trec-covid\n",
      "loading BEIR dataset: webis-touche2020\n",
      "loading BEIR dataset: signal1m\n",
      "loading BEIR dataset: trec-news\n",
      "loading BEIR dataset: robust04\n",
      "loading BEIR dataset: bioasq\n",
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load support set\n",
    "\n",
    "et = corr_trainer.embedder_tokenizer\n",
    "\n",
    "from data_helpers import load_beir_datasets\n",
    "support_set = load_beir_datasets()[\"msmarco\"]\n",
    "\n",
    "support_set = [t[\"text\"] for t in support_set]\n",
    "support_set = [et.decode(et(t, max_length=32)[\"input_ids\"], skip_special_tokens=True) for t in support_set]\n",
    "\n",
    "print(len(support_set))\n",
    "support_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b40bc3a-3ee2-4cb5-a86a-07bd83398c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPR support set: torch.Size([100000, 768])\n",
      "GTR support set: torch.Size([100000, 768])\n"
     ]
    }
   ],
   "source": [
    "# encode support sets\n",
    "\n",
    "import torch\n",
    "support_set_dpr = torch.tensor(dpr.encode(support_set))\n",
    "print(\"DPR support set:\", support_set_dpr.shape)\n",
    "\n",
    "support_set_gtr = []\n",
    "batch_size = 512\n",
    "idx = 0\n",
    "while idx < len(support_set):\n",
    "    tokenized = (\n",
    "        et(\n",
    "            support_set[idx : idx + batch_size], \n",
    "            max_length=32, \n",
    "            truncation=True,\n",
    "            padding=True, \n",
    "            return_tensors='pt'\n",
    "        ).to(corr_trainer.args.device)\n",
    "    )\n",
    "    gtr_embs = corr_trainer.inversion_trainer.call_embedding_model(\n",
    "        # embedder_input_ids=tokenized.input_ids,\n",
    "        # embedder_attention_mask=tokenized.attention_mask,\n",
    "        **tokenized\n",
    "    )\n",
    "    support_set_gtr.extend(gtr_embs.cpu().tolist())\n",
    "    idx += batch_size\n",
    "\n",
    "support_set_gtr = torch.tensor(support_set_gtr)\n",
    "print(\"GTR support set:\", support_set_gtr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4fb83325-5949-4ff0-a99f-d23a89738f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mage (foaled April 18, 2020) is my American Thoroughbred racehorse who won the 2023 Kentucky Derby.\n",
      "tensor(0.0643)\n",
      "['The 2018 Annual Forecast will be available for you to make a profit from your own Investing in the future. Most credit cards have minimum 10 percent']\n"
     ]
    }
   ],
   "source": [
    "# embedding text\n",
    "text = \"Mage (foaled April 18, 2020) is my American Thoroughbred racehorse who won the 2023 Kentucky Derby.\"\n",
    "\n",
    "def invert_with_support(t: str) -> str:\n",
    "    # project into support set\n",
    "    temp = 1 / 50 # 00.\n",
    "    # gtr_emb = corr_trainer.inversion_trainer.call_embedding_model(\n",
    "    #     # embedder_input_ids=tokenized.input_ids,\n",
    "    #     # embedder_attention_mask=tokenized.attention_mask,\n",
    "    #     **et([text], return_tensors='pt').to(corr_trainer.args.device)\n",
    "    # ).cpu()\n",
    "    # gtr_emb = support_set_gtr[None,0]\n",
    "    # print(gtr_emb.shape)\n",
    "    # support_set_scores = torch.nn.functional.cosine_similarity(gtr_emb, support_set_gtr, dim=-1)\n",
    "    dpr_emb = torch.tensor(dpr.encode([t]))\n",
    "    support_set_scores = torch.nn.functional.cosine_similarity(dpr_emb, support_set_dpr, dim=-1)\n",
    "    support_set_dist = (support_set_scores / temp).softmax(0)\n",
    "    gtr_emb = (support_set_dist[None] @ support_set_gtr)\n",
    "    #### import pdb; pdb.set_trace()\n",
    "    print(support_set_dist.max())\n",
    "    gtr_emb = gtr_emb.to(corr_trainer.args.device)\n",
    "    # invert\n",
    "    fake_embedder_input_ids = torch.ones((1, 32), device=corr_trainer.args.device)\n",
    "    fake_embedder_attention_mask = torch.ones((1, 32), device=corr_trainer.args.device)\n",
    "\n",
    "    corr_trainer.inversion_trainer.model.use_frozen_embeddings_as_input = True\n",
    "    preds_sample = corr_trainer.inversion_trainer.generate(\n",
    "        inputs={ \"frozen_embeddings\": gtr_emb, \"embedder_input_ids\": fake_embedder_input_ids, \"embedder_attention_mask\": fake_embedder_attention_mask },\n",
    "        generation_kwargs = {'early_stopping': False,  'num_beams': 1,  'do_sample': False,  'no_repeat_ngram_size': 0, 'max_length': 32 },\n",
    "    )\n",
    "    \n",
    "    return et.batch_decode(preds_sample, skip_special_tokens=True)\n",
    "\n",
    "print(text)\n",
    "print(invert_with_support(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac18fcb-98f1-48a9-acc8-ca65e3f85488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
