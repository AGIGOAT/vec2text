{"eval_loss": 2.2320404052734375, "eval_accuracy": 0.6961016414141414, "eval_pred_num_tokens": 78.33333587646484, "eval_true_num_tokens": 76.46464538574219, "eval_token_set_precision": 0.45427146242929367, "eval_token_set_recall": 0.5781169915408513, "eval_token_set_f1": 0.4927234400862096, "eval_token_set_f1_std": 0.14529063785874147, "eval_n_ngrams_match_1": 25.04040404040404, "eval_n_ngrams_match_2": 9.272727272727273, "eval_n_ngrams_match_3": 4.03030303030303, "eval_num_true_words": 54.535353535353536, "eval_num_pred_words": 53.696969696969695, "eval_bleu_score": 12.791963357786699, "eval_bleu_score_std": 11.371645321755224, "eval_rouge_score": 0.47945760864532616, "eval_exact_match": 0.0, "eval_exact_match_std": 0.0, "eval_emb_cos_sim": 0.9390913844108582, "eval_emb_cos_sim_std": 0.0322682149708271, "eval_perplexity": 9.318860947298617, "eval_runtime": 80.843, "eval_samples_per_second": 1.225, "eval_steps_per_second": 1.225, "_eval_args": {"alias": "openai_msmarco__msl128__100epoch", "num_samples": 99, "return_best_hypothesis": 1, "num_gen_recursive_steps": 50, "sequence_beam_width": 8, "beam_width": 1, "dataset": "climate-fever"}}